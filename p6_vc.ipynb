{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, List\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class Emotions:\n",
    "    name : str\n",
    "    quantity : int\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x : float\n",
    "    y : float\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    origin : Point\n",
    "    end: Point\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    age : str\n",
    "    emotions : List[Emotions]\n",
    "    bounding_box : BoundingBox\n",
    "    image : np.array\n",
    "\n",
    "@dataclass\n",
    "class FaceDetectorResult:\n",
    "    image : np.array \n",
    "    bounding_box : BoundingBox\n",
    "\n",
    "@dataclass\n",
    "class FaceComparatorResult:\n",
    "    similarity : float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AplicaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from numpy import array\n",
    "from typing import List\n",
    "\n",
    "class FaceDetector(ABC):\n",
    "    @abstractmethod\n",
    "    def detect(self, image : array) -> List[FaceDetectorResult]:\n",
    "        pass\n",
    "\n",
    "class FaceQualifier(ABC):\n",
    "    @abstractmethod\n",
    "    def qualify(self, face_detector_result : FaceDetectorResult) -> List[Person]:\n",
    "        pass\n",
    "\n",
    "class FaceComparator(ABC):\n",
    "    @abstractmethod\n",
    "    def qualify(self, first_face : FaceDetectorResult, second_face: FaceDetectorResult) -> List[FaceComparatorResult]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infraestructura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageUtils:\n",
    "    @staticmethod\n",
    "    def crop(image : np.array, bounding_box : BoundingBox) -> np.array:\n",
    "        return image[bounding_box.origin.x:bounding_box.end.x, bounding_box.origin.y:bounding_box.end.y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from retinaface import RetinaFace\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "\n",
    "class ViolaJonesFaceDetector(FaceDetector):\n",
    "    cascPathface = os.path.dirname(\n",
    "        cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "\n",
    "\n",
    "    def detect(self, image : array) -> List[FaceDetectorResult]:\n",
    "        faceCascade = cv2.CascadeClassifier(self.cascPathface)\n",
    "        gray = self._convert_image_to_gray(image)\n",
    "        faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        return self._convert_to_face_detector_result(image, faces)\n",
    "        \n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces : List[array]) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x+w, y+h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    \n",
    "    def _convert_image_to_gray(self, image : array) -> array:\n",
    "        import cv2\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "class RetinafaceFaceDetector(FaceDetector):\n",
    "    def detect(self, image : array) -> List[FaceDetectorResult]:\n",
    "        faces = RetinaFace.detect_faces(image);\n",
    "        return self._convert_to_face_detector_result(image, [faces[key]['facial_area'] for key in faces.keys()])\n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces : List[array]) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(w, h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "\n",
    "class MediaPipeFaceDetector(FaceDetector):\n",
    "    def detect(self, image : array) -> List[FaceDetectorResult]:\n",
    "        BaseOptions = mp.tasks.BaseOptions\n",
    "        FaceDetector = mp.tasks.vision.FaceDetector\n",
    "        FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "        VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "        options = FaceDetectorOptions(\n",
    "            base_options=BaseOptions(model_asset_path='./blaze_face_short_range.tflite'),\n",
    "            running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "        with FaceDetector.create_from_options(options) as detector:\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "            face_detector_result = detector.detect(mp_image)\n",
    "            return self._convert_to_face_detector_result(image, [(x.bounding_box.origin_x, x.bounding_box.origin_y, x.bounding_box.width, x.bounding_box.height) for x in face_detector_result.detections])\n",
    "        \n",
    "    def _convert_to_face_detector_result(self, image, faces : List[array]) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x + w, y + h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfaceFaceQualifier(FaceQualifier):\n",
    "    def qualify(self, face_detector_result : FaceDetectorResult) -> List[Person]:\n",
    "        from deepface import DeepFace\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import cv2\n",
    "        import os\n",
    "        \n",
    "        return [Person(\"42\", [Emotions(\"happy\", 0.42)], face_detector_result.bounding_box, face_detector_result.image)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV2 INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualificationDisplay(ABC):\n",
    "    @abstractmethod\n",
    "    def display(self) -> None:\n",
    "        pass\n",
    "\n",
    "class OpenCVFaceQualificationDisplay(ABC):\n",
    "    def __init__(self, people : List[Person], frame : np.array) -> None:\n",
    "        self.people = people\n",
    "    \n",
    "    def display(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722257.231426       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722257.327831       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722257.394188       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=540, y=394), end=Point(x=795, y=649)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=537, y=392), end=Point(x=793, y=648)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=539, y=394), end=Point(x=794, y=649)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=538, y=400), end=Point(x=788, y=650)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722257.461165       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722257.527534       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722257.595368       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722257.662799       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=536, y=393), end=Point(x=792, y=649)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=532, y=394), end=Point(x=785, y=647)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=529, y=395), end=Point(x=791, y=657)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=538, y=409), end=Point(x=786, y=657)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722257.729295       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722257.794401       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722257.862708       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722257.929147       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=548, y=407), end=Point(x=783, y=642)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=550, y=407), end=Point(x=779, y=636)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722257.993016       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.060246       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.129300       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.195494       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=570, y=409), end=Point(x=793, y=632)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=526, y=406), end=Point(x=762, y=642)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=532, y=405), end=Point(x=772, y=645)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722258.260231       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.327710       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.393676       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.459883       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=524, y=400), end=Point(x=769, y=645)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=530, y=394), end=Point(x=763, y=627)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=525, y=405), end=Point(x=745, y=625)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=524, y=397), end=Point(x=762, y=635)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722258.527364       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.594590       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.662346       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.728228       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=541, y=403), end=Point(x=740, y=602)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=539, y=395), end=Point(x=756, y=612)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=519, y=390), end=Point(x=766, y=637)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=526, y=395), end=Point(x=760, y=629)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722258.794221       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.862046       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.926158       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722258.993362       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=535, y=401), end=Point(x=765, y=631)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=535, y=402), end=Point(x=759, y=626)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=540, y=414), end=Point(x=754, y=628)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=558, y=406), end=Point(x=763, y=611)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722259.061103       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.126233       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.197349       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.259592       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=553, y=406), end=Point(x=766, y=619)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=585, y=408), end=Point(x=775, y=598)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722259.326580       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.397048       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.459812       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.526283       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=553, y=406), end=Point(x=775, y=628)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=553, y=410), end=Point(x=758, y=615)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=563, y=400), end=Point(x=751, y=588)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=535, y=402), end=Point(x=754, y=621)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722259.593051       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.660496       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.727929       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.793512       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=524, y=399), end=Point(x=757, y=632)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=511, y=403), end=Point(x=749, y=641)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=519, y=404), end=Point(x=750, y=635)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=503, y=383), end=Point(x=760, y=640)))]\n",
      "[Person(age='42', emotions=[Emotions(name='happy', quantity=0.42)], bounding_box=BoundingBox(origin=Point(x=502, y=403), end=Point(x=744, y=645)))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701722259.859155       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701722259.926226       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m video \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39mVideoCapture(\u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39;49mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     face_detector \u001b[39m=\u001b[39m MediaPipeFaceDetector()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     faces \u001b[39m=\u001b[39m face_detector\u001b[39m.\u001b[39mdetect(frame)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    face_detector = MediaPipeFaceDetector()\n",
    "    faces = face_detector.detect(frame)\n",
    "    for x in faces:\n",
    "        cv.rectangle(frame, (x.bounding_box.origin.x, x.bounding_box.origin.y), (x.bounding_box.end.x, x.bounding_box.end.y), (255, 0, 0), 2)\n",
    "    \n",
    "    for x in faces:\n",
    "        qualifier = DeepfaceFaceQualifier()\n",
    "        qualification = qualifier.qualify(x)\n",
    "        print(qualification)\n",
    "    cv.imshow('Video', frame)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
