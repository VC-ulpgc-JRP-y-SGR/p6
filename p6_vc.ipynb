{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from retinaface import RetinaFace\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Emotions:\n",
    "    name : str\n",
    "    quantity : int\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x : float\n",
    "    y : float\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    origin : Point\n",
    "    end: Point\n",
    "\n",
    "\n",
    "class Genre:\n",
    "    male = 'MALE'\n",
    "    female = 'FEMALE'\n",
    "\n",
    "    def __init__(self, genre : str):\n",
    "        self.genre = genre\n",
    "\n",
    "    @staticmethod\n",
    "    def female(cls):\n",
    "        return Genre(Genre.female)\n",
    "    \n",
    "    @staticmethod\n",
    "    def male(cls):\n",
    "        return Genre(Genre.male)\n",
    "    \n",
    "    def isMale(self):\n",
    "        return self.genre == Genre.male\n",
    "\n",
    "    def isFemale(self):\n",
    "        return self.genre == Genre.female\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    age : str\n",
    "    genre : Genre\n",
    "    emotions : List[Emotions]\n",
    "    bounding_box : BoundingBox\n",
    "    image : np.ndarray\n",
    "\n",
    "@dataclass\n",
    "class FaceDetectorResult:\n",
    "    image : np.ndarray \n",
    "    bounding_box : BoundingBox\n",
    "\n",
    "@dataclass\n",
    "class FaceComparatorResult:\n",
    "    similarity : float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'nd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 5\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m ABC, abstractmethod\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m List\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mclass\u001b[39;49;00m \u001b[39mFaceDetector\u001b[39;49;00m(ABC):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m@abstractmethod\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mdef\u001b[39;49;00m \u001b[39mdetect\u001b[39;49m(\u001b[39mself\u001b[39;49m, image : np\u001b[39m.\u001b[39;49mnd) \u001b[39m-\u001b[39;49m\u001b[39m>\u001b[39;49m List[FaceDetectorResult]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mpass\u001b[39;49;00m\n",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mFaceDetector\u001b[39;00m(ABC):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m@abstractmethod\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mdetect\u001b[39m(\u001b[39mself\u001b[39m, image : np\u001b[39m.\u001b[39;49mnd) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[FaceDetectorResult]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/numpy/__init__.py:333\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mRemoved in NumPy 1.25.0\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTester was removed in NumPy 1.25.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 333\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m has no attribute \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    334\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'nd'"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "class FaceDetector(ABC):\n",
    "    @abstractmethod\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        pass\n",
    "\n",
    "class FaceQualifier(ABC):\n",
    "    @abstractmethod\n",
    "    def qualify(self, face_detector_result : FaceDetectorResult) -> List[Person]:\n",
    "        pass\n",
    "\n",
    "class FaceComparator(ABC):\n",
    "    @abstractmethod\n",
    "    def qualify(self, first_face : FaceDetectorResult, second_face: FaceDetectorResult) -> FaceComparatorResult:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infraestructura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageUtils:\n",
    "    @staticmethod\n",
    "    def crop(image : np.array, bounding_box : BoundingBox) -> np.ndarray:\n",
    "        return image[bounding_box.origin.x:bounding_box.end.x, bounding_box.origin.y:bounding_box.end.y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViolaJonesFaceDetector(FaceDetector):\n",
    "    cascPathface = os.path.dirname(\n",
    "        cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "\n",
    "\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        faceCascade = cv2.CascadeClassifier(self.cascPathface)\n",
    "        gray = self._convert_image_to_gray(image)\n",
    "        faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        return self._convert_to_face_detector_result(image, faces)\n",
    "        \n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x+w, y+h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    \n",
    "    def _convert_image_to_gray(self, image : np.ndarray) -> np.ndarray:\n",
    "        import cv2\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "class RetinafaceFaceDetector(FaceDetector):\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        faces = RetinaFace.detect_faces(image);\n",
    "        return self._convert_to_face_detector_result(image, [faces[key]['facial_area'] for key in faces.keys()])\n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(w, h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "\n",
    "class MediaPipeFaceDetector(FaceDetector):\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        BaseOptions = mp.tasks.BaseOptions\n",
    "        FaceDetector = mp.tasks.vision.FaceDetector\n",
    "        FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "        VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "        options = FaceDetectorOptions(\n",
    "            base_options=BaseOptions(model_asset_path='./blaze_face_short_range.tflite'),\n",
    "            running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "        with FaceDetector.create_from_options(options) as detector:\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "            face_detector_result = detector.detect(mp_image)\n",
    "            return self._convert_to_face_detector_result(image, [(x.bounding_box.origin_x, x.bounding_box.origin_y, x.bounding_box.width, x.bounding_box.height) for x in face_detector_result.detections])\n",
    "        \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x + w, y + h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockFaceQualifier(FaceQualifier):\n",
    "    def qualify(self, face_detector_result : FaceDetectorResult) -> List[Person]:\n",
    "        return Person(\n",
    "            age=43, \n",
    "            genre=Genre.male(), \n",
    "            emotions=[Emotions('happy', 0.8)], \n",
    "            bounding_box=face_detector_result.bounding_box, \n",
    "            image=face_detector_result.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfaceFaceQualifier(FaceQualifier):\n",
    "    def qualify(self, face_detector_result: FaceDetectorResult) -> List[Person]:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MockFaceComparator(FaceComparator):\n",
    "    def qualify(self, first_face : FaceDetectorResult, second_face: FaceDetectorResult) -> FaceComparatorResult:\n",
    "        return FaceComparatorResult(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfaceFaceComparator(FaceComparator):\n",
    "    def qualify(self, first_face : FaceDetectorResult, second_face: FaceDetectorResult) -> FaceComparatorResult:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV2 INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualificationDisplay(ABC):\n",
    "    @abstractmethod\n",
    "    def display(self) -> None:\n",
    "        pass\n",
    "\n",
    "class OpenCVFaceQualificationDisplay(ABC):\n",
    "    def __init__(self, people : List[Person], frame : np.array) -> None:\n",
    "        self.people = people\n",
    "    \n",
    "    def display(self):\n",
    "        pass\n",
    "    \n",
    "    def paint_interface(self, person : Person, frame: np.ndarray):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthenticationScreen(ABC):\n",
    "    @abstractmethod\n",
    "    def display(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LockAuthenticationScreen(AuthenticationScreen):\n",
    "    def __init__(self, frame : np.ndarray) -> None:\n",
    "        self.frame = frame\n",
    "\n",
    "    def display(self) -> None:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessGrantedAuthenticationScreen(AuthenticationScreen):\n",
    "    def __init__(self, frame : np.ndarray) -> None:\n",
    "        self.frame = frame\n",
    "\n",
    "    def display(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessDeniedAuthenticationScreen(AuthenticationScreen):\n",
    "    def __init__(self, frame : np.ndarray) -> None:\n",
    "        self.frame = frame\n",
    "\n",
    "    def display(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class State:\n",
    "    screen : AuthenticationScreen\n",
    "\n",
    "class AuthenticationStateMachine(ABC):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    face_detector = MediaPipeFaceDetector()\n",
    "    faces = face_detector.detect(frame)\n",
    "    for x in faces:\n",
    "        cv.rectangle(frame, (x.bounding_box.origin.x, x.bounding_box.origin.y), (x.bounding_box.end.x, x.bounding_box.end.y), (255, 0, 0), 2)\n",
    "    \n",
    "    for x in faces:\n",
    "        qualifier = DeepfaceFaceQualifier()\n",
    "        qualification = qualifier.qualify(x)\n",
    "        print(qualification)\n",
    "    cv.imshow('Video', frame)\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
