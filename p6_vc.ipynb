{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sarag\\anaconda3\\envs\\VC_P6\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from retinaface import RetinaFace\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Emotions:\n",
    "    name : str\n",
    "    quantity : int\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x : float\n",
    "    y : float\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    origin : Point\n",
    "    end: Point\n",
    "\n",
    "\n",
    "class Genre:\n",
    "    male = 'Man'\n",
    "    female = 'Woman'\n",
    "\n",
    "    def __init__(self, genre : str):\n",
    "        self.genre = genre\n",
    "\n",
    "    @staticmethod\n",
    "    def female():\n",
    "        return Genre(Genre.female)\n",
    "    \n",
    "    @staticmethod\n",
    "    def male():\n",
    "        return Genre(Genre.male)\n",
    "    \n",
    "    def isMale(self):\n",
    "        return self.genre == Genre.male\n",
    "\n",
    "    def isFemale(self):\n",
    "        return self.genre == Genre.female\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    age : str\n",
    "    genre : Genre\n",
    "    emotions : List[Emotions]\n",
    "    bounding_box : BoundingBox\n",
    "    image : np.ndarray\n",
    "\n",
    "@dataclass\n",
    "class FaceDetectorResult:\n",
    "    image : np.ndarray \n",
    "    bounding_box : BoundingBox\n",
    "\n",
    "@dataclass\n",
    "class FaceComparatorResult:\n",
    "    similarity : float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AplicaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "class FaceDetector(ABC):\n",
    "    @abstractmethod\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        pass\n",
    "\n",
    "class FaceQualifier(ABC):\n",
    "    @abstractmethod\n",
    "    def qualify(self, face_detector_result : FaceDetectorResult) -> Person:\n",
    "        pass\n",
    "\n",
    "class FaceComparator(ABC):\n",
    "    @abstractmethod\n",
    "    def qualify(self, first_face : FaceDetectorResult, second_face: FaceDetectorResult) -> FaceComparatorResult:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infraestructura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageUtils:\n",
    "    @staticmethod\n",
    "    def crop(image : np.array, bounding_box : BoundingBox) -> np.ndarray:\n",
    "        return image[bounding_box.origin.x:bounding_box.end.x, bounding_box.origin.y:bounding_box.end.y]\n",
    "    \n",
    "    @staticmethod\n",
    "    def overlay_icon(image: np.array, icon_path: str, color: tuple, icon_size : int, point : tuple) -> np.ndarray:\n",
    "        icon = cv2.imread(icon_path, cv2.IMREAD_GRAYSCALE)\n",
    "        icon = cv2.resize(icon, (icon_size, icon_size))\n",
    "        mask = icon == 0\n",
    "        color_layer = np.full((icon.shape[0], icon.shape[1], 3), color, dtype=np.uint8)\n",
    "        np.copyto(image[point[1]-icon.shape[0]//2:point[1]+icon.shape[0]//2, point[0]-icon.shape[1]//2:point[0]+icon.shape[1]//2], color_layer, where=mask[:,:,None])\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViolaJonesFaceDetector(FaceDetector):\n",
    "    cascPathface = os.path.dirname(\n",
    "        cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        faceCascade = cv2.CascadeClassifier(self.cascPathface)\n",
    "        gray = self._convert_image_to_gray(image)\n",
    "        faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        return self._convert_to_face_detector_result(image, faces)\n",
    "        \n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x+w, y+h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    \n",
    "    def _convert_image_to_gray(self, image : np.ndarray) -> np.ndarray:\n",
    "        import cv2\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "class RetinafaceFaceDetector(FaceDetector):\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        faces = RetinaFace.detect_faces(image);\n",
    "        return self._convert_to_face_detector_result(image, [faces[key]['facial_area'] for key in faces.keys()])\n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(w, h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "\n",
    "class MediaPipeFaceDetector(FaceDetector):\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        BaseOptions = mp.tasks.BaseOptions\n",
    "        FaceDetector = mp.tasks.vision.FaceDetector\n",
    "        FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "        VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "        options = FaceDetectorOptions(\n",
    "            base_options=BaseOptions(model_asset_path='./blaze_face_short_range.tflite'),\n",
    "            running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "        with FaceDetector.create_from_options(options) as detector:\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "            face_detector_result = detector.detect(mp_image)\n",
    "            return self._convert_to_face_detector_result(image, [(x.bounding_box.origin_x, x.bounding_box.origin_y, x.bounding_box.width, x.bounding_box.height) for x in face_detector_result.detections])\n",
    "        \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x + w, y + h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockFaceQualifier(FaceQualifier):\n",
    "    def qualify(self, face_detector_result):\n",
    "        return Person(\n",
    "            age=43, \n",
    "            genre=Genre.male(), \n",
    "            emotions=[Emotions('happy', 0.8),\n",
    "                      Emotions('sad', 0.5),\n",
    "                      Emotions('angry', 0.3)\n",
    "                      ], \n",
    "            bounding_box=face_detector_result.bounding_box, \n",
    "            image=face_detector_result.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "class DeepfaceFaceQualifier(FaceQualifier):\n",
    "    def qualify(self, face_detector_result: FaceDetectorResult) -> Person:\n",
    "        face_image = face_detector_result.image\n",
    "        deepface_result = DeepFace.analyze(img_path = face_image, actions = ['age', 'gender', 'emotion'], enforce_detection=False)[0]\n",
    "        \n",
    "        age = str(deepface_result[\"age\"])\n",
    "        gender = Genre(max(deepface_result[\"gender\"], key = deepface_result[\"gender\"].get))\n",
    "        emotion_predictions = deepface_result[\"emotion\"]\n",
    "        emotions = [Emotions(name=emotion, quantity=round(probability, 3)) for emotion, probability in emotion_predictions.items()]\n",
    "        sorted_emotions = sorted(emotions, key=lambda x: x.quantity, reverse=True)\n",
    "\n",
    "        person = Person(\n",
    "            age = age,\n",
    "            genre = gender,\n",
    "            emotions = sorted_emotions[0:3],\n",
    "            bounding_box = face_detector_result.bounding_box,\n",
    "            image = face_image\n",
    "        )\n",
    "        \n",
    "        return person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualificationDisplay:\n",
    "    def show():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "class OpenCVFaceQualificationDisplay(FaceQualificationDisplay):\n",
    "    def __init__(self, people : List[Person], frame : np.ndarray) -> None:\n",
    "        self.people = people\n",
    "        self.frame = frame\n",
    "    \n",
    "    def display_emotions_right_of_person(self, person: Person, padding_x: int, padding_y: int) -> None:\n",
    "        width = 200\n",
    "        border_width =2\n",
    "        rectangle_origin = (person.bounding_box.end.x + border_width, person.bounding_box.origin.y - border_width)\n",
    "        rectangle_end = (rectangle_origin[0] + width, person.bounding_box.end.y + border_width)\n",
    "        if rectangle_end[0] > self.frame.shape[1]:\n",
    "            rectangle_end = (self.frame.shape[1], rectangle_end[1])\n",
    "        if rectangle_end[1] > self.frame.shape[0]:\n",
    "            rectangle_end = (rectangle_end[0], self.frame.shape[0])\n",
    "        #blur the rectangle background\n",
    "        self.frame[rectangle_origin[1]:rectangle_end[1], rectangle_origin[0]:rectangle_end[0]] = cv.GaussianBlur(self.frame[rectangle_origin[1]:rectangle_end[1], rectangle_origin[0]:rectangle_end[0]], (75, 75), 0) / 1.5\n",
    "\n",
    "        font_weight=1\n",
    "        font_size=0.8\n",
    "        text = \"Emotions\"\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "        text_width, text_height = text_size\n",
    "        text_x = rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - text_width // 2\n",
    "        text_y = rectangle_origin[1] + text_height + padding_y // 2\n",
    "        cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, (255, 255, 255), font_weight, lineType = cv2.LINE_AA)\n",
    "\n",
    "        gap = 40\n",
    "\n",
    "        for x in person.emotions:\n",
    "            #draw text of emotion\n",
    "            font_weight=1\n",
    "            font_size=0.5\n",
    "            text = x.name + \": \" + str(x.quantity)\n",
    "            text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "            text_width, text_height = text_size\n",
    "            text_x = rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - text_width // 2\n",
    "            text_y = (rectangle_origin[1] + text_height + padding_y // 2) + gap\n",
    "            cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, (255, 255, 255), font_weight, lineType = cv2.LINE_AA)\n",
    "            gap+=30\n",
    "\n",
    "\n",
    "    def display_age_on_top_of_person(self, person: Person, padding_x: int, padding_y: int) -> None:\n",
    "        gap = 26\n",
    "        font_thinkness = 2\n",
    "        icon_size = 38\n",
    "        triangle_size = 20\n",
    "\n",
    "        text = str(person.age)\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, 1.5, font_thinkness)\n",
    "        text_width, text_height = text_size\n",
    "\n",
    "        rectangle_origin = ((person.bounding_box.origin.x + (person.bounding_box.end.x - person.bounding_box.origin.x) // 2 - (text_width + padding_x) // 2), person.bounding_box.origin.y - text_height - padding_y - 50)\n",
    "        rectangle_end = (rectangle_origin[0] + text_width + padding_x, rectangle_origin[1] + text_height + padding_y)\n",
    "\n",
    "        # Draw the rectangle with a pointer\n",
    "        cv.rectangle(self.frame, rectangle_origin, rectangle_end, (67, 193, 246), cv.FILLED)\n",
    "\n",
    "        # Center the text within the rectangle\n",
    "        text_x = rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - text_width // 2\n",
    "        text_y = rectangle_origin[1] + text_height + padding_y // 2\n",
    "\n",
    "        # Draw the text\n",
    "        cv.putText(self.frame, text, (text_x - gap, text_y), cv.FONT_HERSHEY_DUPLEX, 1.5, (47, 123, 222), font_thinkness, lineType = cv2.LINE_AA)\n",
    "\n",
    "        triangle = np.array([[rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - triangle_size + 5, rectangle_end[1]],\n",
    "                            [rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 + triangle_size - 5, rectangle_end[1]],\n",
    "                            [rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2, rectangle_end[1] + triangle_size] ], np.int32)\n",
    "        \n",
    "        cv.drawContours(self.frame, [triangle], 0, (67, 193, 246), cv.FILLED)\n",
    "\n",
    "        male_icon = cv.imread(\"./assets/male.png\" if person.genre.isMale() else \"./assets/female.png\")  # Correct the path to your male icon image\n",
    "        if male_icon is not None:\n",
    "            male_icon = cv.resize(male_icon, (icon_size, icon_size))\n",
    "            rectangle_center = (rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2, rectangle_origin[1] + icon_size // 2 + padding_y//2)\n",
    "            self.overlay_icon(male_icon, (rectangle_center[0] + gap, rectangle_center[1]))\n",
    "        else:\n",
    "            print(\"Male icon not found or there's an error in reading the image.\")\n",
    "    \n",
    "    def overlay_icon(self, icon, center):\n",
    "        \"\"\"Overlay an icon image at the specified top left corner position.\"\"\"\n",
    "        h, w, _ = icon.shape\n",
    "        x, y = center\n",
    "        # Check if the coordinates are within the frame bounds\n",
    "        #draw only if the icon fits inside the frame\n",
    "        if x-w//2 >= 0 and y-h//2 >= 0 and x+w//2 < self.frame.shape[1] and y+h//2 < self.frame.shape[0]:\n",
    "            self.frame[y-h//2:y+h//2, x-w//2:x+w//2] = icon\n",
    "    \n",
    "    def display_box_around_person(self, person : Person) -> None:\n",
    "        cv.rectangle(self.frame, (person.bounding_box.origin.x, \n",
    "                                  person.bounding_box.origin.y), \n",
    "                                  (person.bounding_box.end.x, \n",
    "                                   person.bounding_box.end.y), \n",
    "                                   (67, 193, 246), 2)\n",
    "    \n",
    "    def show(self):\n",
    "        for person in self.people:\n",
    "            self.display_box_around_person(person)\n",
    "            self.display_age_on_top_of_person(person, 130, 38)\n",
    "            self.display_emotions_right_of_person(person, 130, 38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MockFaceComparator(FaceComparator):\n",
    "    def qualify(self, first_face, second_face):\n",
    "        return FaceComparatorResult(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "class DeepfaceFaceComparator(FaceComparator):\n",
    "    def qualify(self, first_face, second_face):\n",
    "        input_image = first_face.image\n",
    "        comparator_image = second_face.image\n",
    "\n",
    "        result = DeepFace.verify(img1_path=input_image, img2_path=comparator_image, enforce_detection=False)\n",
    "        return FaceComparatorResult(result[\"distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV2 INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualificationDisplay(ABC):\n",
    "    @abstractmethod\n",
    "    def show(self) -> None:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:04<00:00,  1.37s/it]\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:01<00:00,  1.64it/s]\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:02<00:00,  1.46it/s]\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:01<00:00,  1.83it/s]\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:01<00:00,  1.91it/s]\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:01<00:00,  2.20it/s]\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:01<00:00,  2.19it/s]\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:01<00:00,  1.61it/s]\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:01<00:00,  1.68it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "face_detector = MediaPipeFaceDetector()\n",
    "qualifier = DeepfaceFaceQualifier()\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    faces = face_detector.detect(frame)\n",
    "    qualifications = [qualifier.qualify(face) for face in faces]\n",
    "    display = OpenCVFaceQualificationDisplay(qualifications, frame)\n",
    "    display.show()\n",
    "\n",
    "    cv.imshow('Video', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACE AUTHENTICATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthenticationScreen(ABC):\n",
    "    @abstractmethod\n",
    "    def display(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    next_state : str\n",
    "    actions: List[Callable[[], bool]]\n",
    "\n",
    "    @staticmethod\n",
    "    def to(next_state : str) -> 'Transition':\n",
    "        return Transition(next_state, [])\n",
    "\n",
    "    def when(self, action : Callable[[], bool]) -> 'Transition':\n",
    "        self.actions.append(action)\n",
    "        return self\n",
    "\n",
    "    def evaluate_transition(self) -> bool:\n",
    "        for action in self.actions:\n",
    "            if action():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "@dataclass\n",
    "class State:\n",
    "    is_initial : bool\n",
    "    name : str\n",
    "    screen : Callable[[np.ndarray], AuthenticationScreen]\n",
    "    transitions : List[Transition]\n",
    "    on_enter : Callable[[], None] = lambda : None\n",
    "    on_exit : Callable[[], None] = lambda : None\n",
    "\n",
    "    @staticmethod\n",
    "    def default(name : str, screen : Callable[[np.ndarray], AuthenticationScreen]) -> 'State':\n",
    "        return State(True, name, screen, [])\n",
    "    \n",
    "    @staticmethod\n",
    "    def of(name : str, screen : Callable[[np.ndarray], AuthenticationScreen]) -> 'State':\n",
    "        return State(False, name, screen, [])\n",
    "\n",
    "    def do(self, transition : Transition) -> 'State':\n",
    "        self.transitions.append(transition)\n",
    "        return self\n",
    "    \n",
    "    def do_on_enter(self, action : Callable[[], None]) -> 'State':\n",
    "        self.on_enter = action\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class StateMachine(ABC):\n",
    "    class Builder:\n",
    "        def __init__(self) -> None:\n",
    "            self.states = []\n",
    "        \n",
    "        def add_state(self, state : State) -> 'Builder':\n",
    "            self.states.append(state)\n",
    "            return self\n",
    "        \n",
    "        def build(self) -> 'StateMachine':\n",
    "            return StateMachine(self.states)\n",
    "    \n",
    "    @staticmethod\n",
    "    def start_building() -> Builder:\n",
    "        return StateMachine.Builder()\n",
    "\n",
    "    def __init__(self, states: List[State] ) -> None:\n",
    "        self.states = states\n",
    "        self.current_state = list(filter(lambda x: x.is_initial, states))[0]\n",
    "        self.current_state.on_enter()\n",
    "    \n",
    "    def execute(self, frame : np.array):\n",
    "        self.current_state.screen(frame).display()\n",
    "        self.evaluate_conditions()\n",
    "    \n",
    "    def evaluate_conditions(self) -> None:\n",
    "        for transition in self.current_state.transitions:\n",
    "            if transition.evaluate_transition():\n",
    "                self.current_state.on_exit()\n",
    "                self.current_state = list(filter(lambda x: x.name == transition.next_state, self.states))[0]\n",
    "                self.current_state.on_enter()\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFRAESTRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IconAuthenticationScreen(AuthenticationScreen):\n",
    "    def __init__(self, frame : np.ndarray, icon_path : str, color : tuple, text : str, bg_color : str = None, alpha : float = 0.15) -> None:\n",
    "        self.frame = frame\n",
    "        self.icon_path = icon_path\n",
    "        self.color = color\n",
    "        self.text = text\n",
    "        self.bg_color = self.color if(bg_color == None) else bg_color\n",
    "        self.alpha  = alpha\n",
    "\n",
    "    def overlay_green_screen(self):\n",
    "        # Create a green screen of the same size as the frame\n",
    "        screen = np.full(self.frame.shape, self.bg_color, dtype=np.uint8)\n",
    "        alpha = self.alpha\n",
    "        self.frame[:,:] = cv2.addWeighted(self.frame, 1 - alpha, screen, alpha, 0)\n",
    "\n",
    "    def display(self) -> None:\n",
    "        self.overlay_green_screen()\n",
    "        ImageUtils.overlay_icon(image=self.frame, \n",
    "                                icon_path=self.icon_path, \n",
    "                                color=self.color, \n",
    "                                icon_size=200, \n",
    "                                point=(self.frame.shape[1] // 2, self.frame.shape[0] // 2))\n",
    "        #draw text that says Press any key to unlock\n",
    "        font_weight=2\n",
    "        font_size=1.2\n",
    "        text = self.text\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "        text_width, text_height = text_size\n",
    "        text_x = self.frame.shape[1] // 2 - text_width // 2\n",
    "        text_y = self.frame.shape[0] // 2 + text_height + 150\n",
    "        cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, self.color, font_weight, lineType = cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LockAuthenticationScreen(IconAuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray) -> None:\n",
    "        super().__init__(frame, \"./assets/lock.png\", (255, 255, 255), \"PRESS A KEY TO START FACE RECOGNITION\", bg_color=(0, 0, 0), alpha=0.4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessGrantedAuthenticationScreen(IconAuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray) -> None:\n",
    "        super().__init__(frame, \"./assets/success.png\", (0, 255, 0), \"ACCESS GRANTED\", bg_color=(0, 255, 0), alpha = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessDeniedAuthenticationScreen(IconAuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray) -> None:\n",
    "        super().__init__(frame, \"./assets/danger.png\", (0, 0, 255), \"ACCESS DENIED\", bg_color=(0, 0, 255), alpha = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "class FaceRecognizerAuthenticationScreen(AuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray ) -> None:\n",
    "        self.frame = frame\n",
    "\n",
    "    def display(self) -> None:\n",
    "        self.draw_landmarks()\n",
    "        self.draw_processing_text()\n",
    "    \n",
    "    def draw_processing_text(self):\n",
    "        font_weight=2\n",
    "        font_size=1.2\n",
    "        text = \"PROCESSING FACE\"\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "        text_width, text_height = text_size\n",
    "        text_x = self.frame.shape[1] // 2 - text_width // 2\n",
    "        text_y = 300 \n",
    "        cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, (255, 255, 255), font_weight, lineType = cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    def draw_landmarks(self):\n",
    "        base_options = python.BaseOptions(model_asset_path='./face_landmarker.task')\n",
    "        options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                            output_face_blendshapes=True,\n",
    "                                            output_facial_transformation_matrixes=True)\n",
    "        detector = vision.FaceLandmarker.create_from_options(options)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=self.frame)\n",
    "        detection_result = detector.detect(mp_image)\n",
    "        self.frame[::] = self.draw_landmarks_on_image(self.frame, detection_result)\n",
    "    \n",
    "    def draw_landmarks_on_image(self, rgb_image, detection_result):\n",
    "        face_landmarks_list = detection_result.face_landmarks\n",
    "        annotated_image = np.copy(rgb_image)\n",
    "\n",
    "        # Loop through the detected faces to visualize.\n",
    "        for idx in range(len(face_landmarks_list)):\n",
    "            face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "            # Draw the face landmarks.\n",
    "            face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "            face_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "            ])\n",
    "\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles\n",
    "                .get_default_face_mesh_tesselation_style())\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles\n",
    "                .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "        return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputManager:\n",
    "    @staticmethod\n",
    "    def is_space_pressed() -> bool:\n",
    "        return cv.waitKey(1) & 0xFF == ord(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATE MACHINE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import threading\n",
    "import random\n",
    "\n",
    "#exist a timer\n",
    "is_authorizated = False\n",
    "finished_recognition = False\n",
    "\n",
    "def set_authorizated():\n",
    "    global is_authorizated\n",
    "    global finished_recognition\n",
    "    is_authorizated = random.choice([True, False])\n",
    "    finished_recognition=True\n",
    "\n",
    "\n",
    "timer = threading.Timer(5.0, lambda: set_authorizated())\n",
    "\n",
    "def is_access_granted() -> bool:\n",
    "    global is_authorizated\n",
    "    global finished_recognition\n",
    "    return is_authorizated and finished_recognition\n",
    "\n",
    "def is_access_denied() -> bool:\n",
    "    global is_authorizated\n",
    "    global finished_recognition\n",
    "    return not is_authorizated and finished_recognition\n",
    "\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "\n",
    "state_machine = StateMachine.start_building().add_state(\n",
    "        State.default(\n",
    "            name=\"LOCK\", \n",
    "            screen=LockAuthenticationScreen)\n",
    "                    .do(Transition.to(\"RECOGNIZING\").when(InputManager.is_space_pressed))\n",
    "    ).add_state(\n",
    "        State.of( \n",
    "            name=\"RECOGNIZING\", \n",
    "            screen=FaceRecognizerAuthenticationScreen)\n",
    "                .do(Transition.to(\"GRANTED\").when(is_access_granted))\n",
    "                .do_on_enter(timer.start)\n",
    "    ).add_state(\n",
    "        State.of(\n",
    "            name=\"GRANTED\", \n",
    "            screen =AccessGrantedAuthenticationScreen)\n",
    "                    .do(Transition.to(\"LOCK\").when(InputManager.is_space_pressed))\n",
    "    ).add_state(\n",
    "\n",
    "    State.of(\n",
    "          name=\"DENIED\", \n",
    "          screen=AccessDeniedAuthenticationScreen)\n",
    "            .do(Transition.to(\"LOCK\").when(InputManager.is_space_pressed))\n",
    "    ).build()\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    state_machine.execute(frame)\n",
    "    cv.imshow('Video', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'verified': True, 'distance': 0.3851291550029333, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 155, 'h': 155}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.63}\n",
      "0.3851291550029333\n",
      "{'verified': True, 'distance': 0.29359458919159276, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 153, 'h': 153}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.41}\n",
      "0.29359458919159276\n",
      "{'verified': True, 'distance': 0.2542079004406306, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 155, 'h': 155}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.34}\n",
      "0.2542079004406306\n",
      "{'verified': True, 'distance': 0.22012437932947004, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 162, 'h': 162}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.49}\n",
      "0.22012437932947004\n",
      "{'verified': True, 'distance': 0.20258957910302977, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 163, 'h': 163}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.42}\n",
      "0.20258957910302977\n",
      "{'verified': True, 'distance': 0.20462897475977915, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 161, 'h': 161}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.27}\n",
      "0.20462897475977915\n",
      "{'verified': True, 'distance': 0.16996584685965466, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 163, 'h': 163}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.36}\n",
      "0.16996584685965466\n",
      "{'verified': True, 'distance': 0.18307645521198046, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 160, 'h': 160}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.57}\n",
      "0.18307645521198046\n",
      "{'verified': True, 'distance': 0.1688714296279622, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 166, 'h': 166}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.31}\n",
      "0.1688714296279622\n",
      "{'verified': True, 'distance': 0.18471382873969389, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 169, 'h': 169}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.33}\n",
      "0.18471382873969389\n",
      "{'verified': True, 'distance': 0.19866707752592472, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 169, 'h': 169}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.25}\n",
      "0.19866707752592472\n",
      "{'verified': True, 'distance': 0.1773342836027758, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 167, 'h': 167}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.63}\n",
      "0.1773342836027758\n",
      "{'verified': True, 'distance': 0.19404712584682138, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 168, 'h': 168}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.63}\n",
      "0.19404712584682138\n",
      "{'verified': True, 'distance': 0.1859073569262366, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 166, 'h': 166}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.36}\n",
      "0.1859073569262366\n",
      "{'verified': True, 'distance': 0.21988122038230262, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 163, 'h': 163}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.57}\n",
      "0.21988122038230262\n",
      "{'verified': True, 'distance': 0.22947683695894994, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 168, 'h': 168}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.37}\n",
      "0.22947683695894994\n",
      "{'verified': True, 'distance': 0.22676695585991324, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 165, 'h': 165}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.45}\n",
      "0.22676695585991324\n",
      "{'verified': True, 'distance': 0.24127889819455073, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 169, 'h': 169}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.46}\n",
      "0.24127889819455073\n",
      "{'verified': True, 'distance': 0.1800733232408418, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 166, 'h': 166}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.65}\n",
      "0.1800733232408418\n",
      "{'verified': True, 'distance': 0.209999464035174, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 164, 'h': 164}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.65}\n",
      "0.209999464035174\n",
      "{'verified': True, 'distance': 0.19290944519440056, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 164, 'h': 164}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.32}\n",
      "0.19290944519440056\n",
      "{'verified': True, 'distance': 0.21774476276239219, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 165, 'h': 165}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.72}\n",
      "0.21774476276239219\n",
      "{'verified': True, 'distance': 0.18634384532126413, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 166, 'h': 166}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.39}\n",
      "0.18634384532126413\n",
      "{'verified': True, 'distance': 0.22448034029375774, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 163, 'h': 163}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.56}\n",
      "0.22448034029375774\n",
      "{'verified': True, 'distance': 0.22433254688361115, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 163, 'h': 163}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.55}\n",
      "0.22433254688361115\n",
      "{'verified': True, 'distance': 0.21157057179821004, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 164, 'h': 164}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.39}\n",
      "0.21157057179821004\n",
      "{'verified': True, 'distance': 0.21063233885749388, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 163, 'h': 163}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.45}\n",
      "0.21063233885749388\n",
      "{'verified': True, 'distance': 0.19872937758991338, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 163, 'h': 163}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.33}\n",
      "0.19872937758991338\n",
      "{'verified': True, 'distance': 0.2204191647484519, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 165, 'h': 165}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.28}\n",
      "0.2204191647484519\n",
      "{'verified': True, 'distance': 0.1709500064485936, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 166, 'h': 166}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.55}\n",
      "0.1709500064485936\n",
      "{'verified': True, 'distance': 0.20845498092922765, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 167, 'h': 167}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.4}\n",
      "0.20845498092922765\n",
      "{'verified': True, 'distance': 0.2620720721944403, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 162, 'h': 162}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.4}\n",
      "0.2620720721944403\n",
      "{'verified': True, 'distance': 0.25991838227467134, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 164, 'h': 164}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.45}\n",
      "0.25991838227467134\n",
      "{'verified': True, 'distance': 0.2147629263642662, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 159, 'h': 159}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.55}\n",
      "0.2147629263642662\n",
      "{'verified': True, 'distance': 0.17147686523507077, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 149, 'h': 149}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.48}\n",
      "0.17147686523507077\n",
      "{'verified': True, 'distance': 0.1836900878830401, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 159, 'h': 159}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.43}\n",
      "0.1836900878830401\n",
      "{'verified': True, 'distance': 0.22387580614854508, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 155, 'h': 155}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.32}\n",
      "0.22387580614854508\n",
      "{'verified': True, 'distance': 0.2835921100325921, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 156, 'h': 156}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.45}\n",
      "0.2835921100325921\n",
      "{'verified': True, 'distance': 0.32685570198907266, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 168, 'h': 168}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.4}\n",
      "0.32685570198907266\n",
      "{'verified': True, 'distance': 0.31254021391537823, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 161, 'h': 161}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.39}\n",
      "0.31254021391537823\n",
      "{'verified': True, 'distance': 0.33221525318312284, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 168, 'h': 168}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.28}\n",
      "0.33221525318312284\n",
      "{'verified': True, 'distance': 0.3135729601386458, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 167, 'h': 167}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.51}\n",
      "0.3135729601386458\n",
      "{'verified': True, 'distance': 0.3270692893001883, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 162, 'h': 162}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.44}\n",
      "0.3270692893001883\n",
      "{'verified': False, 'distance': 0.412173437761461, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 139, 'h': 139}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.41}\n",
      "0.412173437761461\n",
      "{'verified': True, 'distance': 0.2957697597261306, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 105, 'h': 105}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.5}\n",
      "0.2957697597261306\n",
      "{'verified': True, 'distance': 0.23366473928818055, 'threshold': 0.4, 'model': 'VGG-Face', 'detector_backend': 'opencv', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 0, 'w': 165, 'h': 165}, 'img2': {'x': 0, 'y': 0, 'w': 410, 'h': 634}}, 'time': 1.61}\n",
      "0.23366473928818055\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "comparator = DeepfaceFaceComparator()\n",
    "detector = MediaPipeFaceDetector()\n",
    "\n",
    "img = cv.imread(\"./faces/comparator/5.jpeg\") \n",
    "face_comparer = face_detector.detect(img)[0]\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    faces = face_detector.detect(frame)\n",
    "\n",
    "    for face in faces:\n",
    "        result = comparator.qualify(face, face_comparer)\n",
    "        print(result.similarity)\n",
    "\n",
    "    cv.imshow('Video', frame)\n",
    "    \n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
