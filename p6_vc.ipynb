{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from retinaface import RetinaFace\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Emotions:\n",
    "    name : str\n",
    "    quantity : int\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x : float\n",
    "    y : float\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    origin : Point\n",
    "    end: Point\n",
    "\n",
    "\n",
    "class Genre:\n",
    "    male = 'MALE'\n",
    "    female = 'FEMALE'\n",
    "\n",
    "    def __init__(self, genre : str):\n",
    "        self.genre = genre\n",
    "\n",
    "    @staticmethod\n",
    "    def female():\n",
    "        return Genre(Genre.female)\n",
    "    \n",
    "    @staticmethod\n",
    "    def male():\n",
    "        return Genre(Genre.male)\n",
    "    \n",
    "    def isMale(self):\n",
    "        return self.genre == Genre.male\n",
    "\n",
    "    def isFemale(self):\n",
    "        return self.genre == Genre.female\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    age : str\n",
    "    genre : Genre\n",
    "    emotions : List[Emotions]\n",
    "    bounding_box : BoundingBox\n",
    "    image : np.ndarray\n",
    "\n",
    "@dataclass\n",
    "class FaceDetectorResult:\n",
    "    image : np.ndarray \n",
    "    bounding_box : BoundingBox\n",
    "\n",
    "@dataclass\n",
    "class FaceComparatorResult:\n",
    "    similarity : float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AplicaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "class FaceDetector(ABC):\n",
    "    @abstractmethod\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        pass\n",
    "\n",
    "class FaceQualifier(ABC):\n",
    "    @abstractmethod\n",
    "    def qualify(self, face_detector_result : FaceDetectorResult) -> Person:\n",
    "        pass\n",
    "\n",
    "class FaceComparator(ABC):\n",
    "    @abstractmethod\n",
    "    def qualify(self, first_face : FaceDetectorResult, second_face: FaceDetectorResult) -> FaceComparatorResult:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infraestructura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageUtils:\n",
    "    @staticmethod\n",
    "    def crop(image : np.array, bounding_box : BoundingBox) -> np.ndarray:\n",
    "        return image[bounding_box.origin.x:bounding_box.end.x, bounding_box.origin.y:bounding_box.end.y]\n",
    "    \n",
    "    @staticmethod\n",
    "    def overlay_icon(image: np.array, icon_path: str, color: tuple, icon_size : int, point : tuple) -> np.ndarray:\n",
    "        icon = cv2.imread(icon_path, cv2.IMREAD_GRAYSCALE)\n",
    "        icon = cv2.resize(icon, (icon_size, icon_size))\n",
    "        mask = icon == 0\n",
    "        color_layer = np.full((icon.shape[0], icon.shape[1], 3), color, dtype=np.uint8)\n",
    "        np.copyto(image[point[1]-icon.shape[0]//2:point[1]+icon.shape[0]//2, point[0]-icon.shape[1]//2:point[0]+icon.shape[1]//2], color_layer, where=mask[:,:,None])\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViolaJonesFaceDetector(FaceDetector):\n",
    "    cascPathface = os.path.dirname(\n",
    "        cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        faceCascade = cv2.CascadeClassifier(self.cascPathface)\n",
    "        gray = self._convert_image_to_gray(image)\n",
    "        faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        return self._convert_to_face_detector_result(image, faces)\n",
    "        \n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x+w, y+h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    \n",
    "    def _convert_image_to_gray(self, image : np.ndarray) -> np.ndarray:\n",
    "        import cv2\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "class RetinafaceFaceDetector(FaceDetector):\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        faces = RetinaFace.detect_faces(image);\n",
    "        return self._convert_to_face_detector_result(image, [faces[key]['facial_area'] for key in faces.keys()])\n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(w, h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "\n",
    "class MediaPipeFaceDetector(FaceDetector):\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        BaseOptions = mp.tasks.BaseOptions\n",
    "        FaceDetector = mp.tasks.vision.FaceDetector\n",
    "        FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "        VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "        options = FaceDetectorOptions(\n",
    "            base_options=BaseOptions(model_asset_path='./blaze_face_short_range.tflite'),\n",
    "            running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "        with FaceDetector.create_from_options(options) as detector:\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "            face_detector_result = detector.detect(mp_image)\n",
    "            return self._convert_to_face_detector_result(image, [(x.bounding_box.origin_x, x.bounding_box.origin_y, x.bounding_box.width, x.bounding_box.height) for x in face_detector_result.detections])\n",
    "        \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x + w, y + h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockFaceQualifier(FaceQualifier):\n",
    "    def qualify(self, face_detector_result):\n",
    "        return Person(\n",
    "            age=43, \n",
    "            genre=Genre.male(), \n",
    "            emotions=[Emotions('happy', 0.8),\n",
    "                      Emotions('sad', 0.5),\n",
    "                      Emotions('angry', 0.3)\n",
    "                      ], \n",
    "            bounding_box=face_detector_result.bounding_box, \n",
    "            image=face_detector_result.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "class DeepfaceFaceQualifier(FaceQualifier):\n",
    "    def qualify(self, face_detector_result: FaceDetectorResult) -> Person:\n",
    "        face_image = face_detector_result.image\n",
    "        deepface_result = DeepFace.analyze(img_path = face_image, actions = ['age', 'gender', 'emotion'], enforce_detection=False)[0]\n",
    "        \n",
    "        age = str(deepface_result[\"age\"])\n",
    "        gender = Genre( \"MALE\" if deepface_result[\"dominant_gender\"] == \"Man\" else \"FEMALE\")\n",
    "        emotion_predictions = deepface_result[\"emotion\"]\n",
    "        emotions = [Emotions(name=emotion, quantity=round(probability, 3)) for emotion, probability in emotion_predictions.items()]\n",
    "        sorted_emotions = sorted(emotions, key=lambda x: x.quantity, reverse=True)\n",
    "\n",
    "        person = Person(\n",
    "            age = age,\n",
    "            genre = gender,\n",
    "            emotions = sorted_emotions[0:3],\n",
    "            bounding_box = face_detector_result.bounding_box,\n",
    "            image = face_image\n",
    "        )\n",
    "        \n",
    "        return person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualificationDisplay:\n",
    "    def show():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "class OpenCVFaceQualificationDisplay(FaceQualificationDisplay):\n",
    "    def __init__(self, people : List[Person], frame : np.ndarray) -> None:\n",
    "        self.people = people\n",
    "        self.frame = frame\n",
    "    \n",
    "    def display_emotions_right_of_person(self, person: Person, padding_x: int, padding_y: int) -> None:\n",
    "        width = 200\n",
    "        border_width =2\n",
    "        rectangle_origin = (person.bounding_box.end.x + border_width, person.bounding_box.origin.y - border_width)\n",
    "        rectangle_end = (rectangle_origin[0] + width, person.bounding_box.end.y + border_width)\n",
    "        if rectangle_end[0] > self.frame.shape[1]:\n",
    "            rectangle_end = (self.frame.shape[1], rectangle_end[1])\n",
    "        if rectangle_end[1] > self.frame.shape[0]:\n",
    "            rectangle_end = (rectangle_end[0], self.frame.shape[0])\n",
    "        #blur the rectangle background\n",
    "        self.frame[rectangle_origin[1]:rectangle_end[1], rectangle_origin[0]:rectangle_end[0]] = cv.GaussianBlur(self.frame[rectangle_origin[1]:rectangle_end[1], rectangle_origin[0]:rectangle_end[0]], (75, 75), 0) / 1.5\n",
    "\n",
    "        font_weight=1\n",
    "        font_size=0.8\n",
    "        text = \"Emotions\"\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "        text_width, text_height = text_size\n",
    "        text_x = rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - text_width // 2\n",
    "        text_y = rectangle_origin[1] + text_height + padding_y // 2\n",
    "        cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, (255, 255, 255), font_weight, lineType = cv2.LINE_AA)\n",
    "\n",
    "        gap = 40\n",
    "\n",
    "        for x in person.emotions:\n",
    "            #draw text of emotion\n",
    "            font_weight=1\n",
    "            font_size=0.5\n",
    "            text = x.name + \": \" + str(x.quantity)\n",
    "            text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "            text_width, text_height = text_size\n",
    "            text_x = rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - text_width // 2\n",
    "            text_y = (rectangle_origin[1] + text_height + padding_y // 2) + gap\n",
    "            cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, (255, 255, 255), font_weight, lineType = cv2.LINE_AA)\n",
    "            gap+=30\n",
    "\n",
    "\n",
    "    def display_age_on_top_of_person(self, person: Person, padding_x: int, padding_y: int) -> None:\n",
    "        gap = 26\n",
    "        font_thinkness = 2\n",
    "        icon_size = 38\n",
    "        triangle_size = 20\n",
    "\n",
    "        text = str(person.age)\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, 1.5, font_thinkness)\n",
    "        text_width, text_height = text_size\n",
    "\n",
    "        rectangle_origin = ((person.bounding_box.origin.x + (person.bounding_box.end.x - person.bounding_box.origin.x) // 2 - (text_width + padding_x) // 2), person.bounding_box.origin.y - text_height - padding_y - 50)\n",
    "        rectangle_end = (rectangle_origin[0] + text_width + padding_x, rectangle_origin[1] + text_height + padding_y)\n",
    "\n",
    "        # Draw the rectangle with a pointer\n",
    "        cv.rectangle(self.frame, rectangle_origin, rectangle_end, (67, 193, 246), cv.FILLED)\n",
    "\n",
    "        # Center the text within the rectangle\n",
    "        text_x = rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - text_width // 2\n",
    "        text_y = rectangle_origin[1] + text_height + padding_y // 2\n",
    "\n",
    "        # Draw the text\n",
    "        cv.putText(self.frame, text, (text_x - gap, text_y), cv.FONT_HERSHEY_DUPLEX, 1.5, (47, 123, 222), font_thinkness, lineType = cv2.LINE_AA)\n",
    "\n",
    "        triangle = np.array([[rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - triangle_size + 5, rectangle_end[1]],\n",
    "                            [rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 + triangle_size - 5, rectangle_end[1]],\n",
    "                            [rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2, rectangle_end[1] + triangle_size] ], np.int32)\n",
    "        \n",
    "        cv.drawContours(self.frame, [triangle], 0, (67, 193, 246), cv.FILLED)\n",
    "\n",
    "        male_icon = cv.imread(\"./assets/male.png\" if person.genre.isMale() else \"./assets/female.png\")  # Correct the path to your male icon image\n",
    "        if male_icon is not None:\n",
    "            male_icon = cv.resize(male_icon, (icon_size, icon_size))\n",
    "            rectangle_center = (rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2, rectangle_origin[1] + icon_size // 2 + padding_y//2)\n",
    "            self.overlay_icon(male_icon, (rectangle_center[0] + gap, rectangle_center[1]))\n",
    "        else:\n",
    "            print(\"Male icon not found or there's an error in reading the image.\")\n",
    "    \n",
    "    def overlay_icon(self, icon, center):\n",
    "        \"\"\"Overlay an icon image at the specified top left corner position.\"\"\"\n",
    "        h, w, _ = icon.shape\n",
    "        x, y = center\n",
    "        # Check if the coordinates are within the frame bounds\n",
    "        #draw only if the icon fits inside the frame\n",
    "        if x-w//2 >= 0 and y-h//2 >= 0 and x+w//2 < self.frame.shape[1] and y+h//2 < self.frame.shape[0]:\n",
    "            self.frame[y-h//2:y+h//2, x-w//2:x+w//2] = icon\n",
    "    \n",
    "    def display_box_around_person(self, person : Person) -> None:\n",
    "        cv.rectangle(self.frame, (person.bounding_box.origin.x, \n",
    "                                  person.bounding_box.origin.y), \n",
    "                                  (person.bounding_box.end.x, \n",
    "                                   person.bounding_box.end.y), \n",
    "                                   (67, 193, 246), 2)\n",
    "    \n",
    "    def show(self):\n",
    "        for person in self.people:\n",
    "            self.display_box_around_person(person)\n",
    "            self.display_age_on_top_of_person(person, 130, 38)\n",
    "            self.display_emotions_right_of_person(person, 130, 38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MockFaceComparator(FaceComparator):\n",
    "    def qualify(self, first_face, second_face):\n",
    "        return FaceComparatorResult(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfaceFaceComparator(FaceComparator):\n",
    "    def qualify(self, first_face, second_face):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV2 INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualificationDisplay(ABC):\n",
    "    @abstractmethod\n",
    "    def show(self) -> None:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701814309.507106       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  6.33it/s]\n",
      "I0000 00:00:1701814310.106014       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  6.00it/s]\n",
      "I0000 00:00:1701814310.739529       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  6.89it/s]\n",
      "I0000 00:00:1701814311.304253       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  6.94it/s]\n",
      "I0000 00:00:1701814311.836404       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  6.58it/s]\n",
      "I0000 00:00:1701814312.371817       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  6.57it/s]\n",
      "I0000 00:00:1701814312.938298       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.22it/s]\n",
      "I0000 00:00:1701814313.471997       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.12it/s]\n",
      "I0000 00:00:1701814314.003627       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.26it/s]\n",
      "I0000 00:00:1701814314.501902       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.25it/s]\n",
      "I0000 00:00:1701814315.004273       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.40it/s]\n",
      "I0000 00:00:1701814315.537711       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.30it/s]\n",
      "I0000 00:00:1701814316.070740       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.30it/s]\n",
      "I0000 00:00:1701814316.603139       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.21it/s]\n",
      "I0000 00:00:1701814317.136416       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.23it/s]\n",
      "I0000 00:00:1701814317.669856       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.27it/s]\n",
      "I0000 00:00:1701814318.169467       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.31it/s]\n",
      "I0000 00:00:1701814318.669344       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.15it/s]\n",
      "I0000 00:00:1701814319.202657       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.13it/s]\n",
      "I0000 00:00:1701814319.736783       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  6.56it/s]\n",
      "I0000 00:00:1701814320.301769       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.19it/s]\n",
      "I0000 00:00:1701814320.834848       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.20it/s]\n",
      "I0000 00:00:1701814321.366376       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.35it/s]\n",
      "I0000 00:00:1701814321.901702       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.27it/s]\n",
      "I0000 00:00:1701814322.434053       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.17it/s]\n",
      "I0000 00:00:1701814322.966939       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.28it/s]\n",
      "I0000 00:00:1701814323.500725       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.36it/s]\n",
      "I0000 00:00:1701814324.032966       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.20it/s]\n",
      "I0000 00:00:1701814324.567261       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.32it/s]\n",
      "I0000 00:00:1701814325.100221       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.27it/s]\n",
      "I0000 00:00:1701814325.633465       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.32it/s]\n",
      "I0000 00:00:1701814326.165764       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.12it/s]\n",
      "I0000 00:00:1701814326.699335       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  7.03it/s]\n",
      "I0000 00:00:1701814327.230622       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  6.75it/s]\n",
      "I0000 00:00:1701814327.799738       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  6.58it/s]\n",
      "I0000 00:00:1701814328.398457       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|ââââââââââ| 3/3 [00:00<00:00,  5.38it/s]\n",
      "I0000 00:00:1701814329.063978       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion:  67%|âââââââ   | 2/3 [00:00<00:00,  4.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ret, frame \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m faces \u001b[39m=\u001b[39m face_detector\u001b[39m.\u001b[39mdetect(frame)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m qualifications \u001b[39m=\u001b[39m [qualifier\u001b[39m.\u001b[39;49mqualify(face) \u001b[39mfor\u001b[39;49;00m face \u001b[39min\u001b[39;49;00m faces]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m display \u001b[39m=\u001b[39m OpenCVFaceQualificationDisplay(qualifications, frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m display\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m ret, frame \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m faces \u001b[39m=\u001b[39m face_detector\u001b[39m.\u001b[39mdetect(frame)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m qualifications \u001b[39m=\u001b[39m [qualifier\u001b[39m.\u001b[39;49mqualify(face) \u001b[39mfor\u001b[39;00m face \u001b[39min\u001b[39;00m faces]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m display \u001b[39m=\u001b[39m OpenCVFaceQualificationDisplay(qualifications, frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m display\u001b[39m.\u001b[39mshow()\n",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 17\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqualify\u001b[39m(\u001b[39mself\u001b[39m, face_detector_result: FaceDetectorResult) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Person:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     face_image \u001b[39m=\u001b[39m face_detector_result\u001b[39m.\u001b[39mimage\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     deepface_result \u001b[39m=\u001b[39m DeepFace\u001b[39m.\u001b[39;49manalyze(img_path \u001b[39m=\u001b[39;49m face_image, actions \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39mage\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mgender\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39memotion\u001b[39;49m\u001b[39m'\u001b[39;49m], enforce_detection\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     age \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(deepface_result[\u001b[39m\"\u001b[39m\u001b[39mage\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y101sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     gender \u001b[39m=\u001b[39m Genre( \u001b[39m\"\u001b[39m\u001b[39mMALE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m deepface_result[\u001b[39m\"\u001b[39m\u001b[39mdominant_gender\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMan\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mFEMALE\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/deepface/DeepFace.py:336\u001b[0m, in \u001b[0;36manalyze\u001b[0;34m(img_path, actions, enforce_detection, detector_backend, align, silent)\u001b[0m\n\u001b[1;32m    333\u001b[0m img_gray \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(img_gray, (\u001b[39m48\u001b[39m, \u001b[39m48\u001b[39m))\n\u001b[1;32m    334\u001b[0m img_gray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(img_gray, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m--> 336\u001b[0m emotion_predictions \u001b[39m=\u001b[39m models[\u001b[39m\"\u001b[39;49m\u001b[39memotion\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mpredict(img_gray, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[\u001b[39m0\u001b[39m, :]\n\u001b[1;32m    338\u001b[0m sum_of_predictions \u001b[39m=\u001b[39m emotion_predictions\u001b[39m.\u001b[39msum()\n\u001b[1;32m    340\u001b[0m obj[\u001b[39m\"\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/engine/training.py:2596\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2587\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2588\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2589\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2590\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2593\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2594\u001b[0m         )\n\u001b[0;32m-> 2596\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   2597\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   2598\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2599\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   2600\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2601\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2602\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2603\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2604\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2605\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2606\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m   2607\u001b[0m )\n\u001b[1;32m   2609\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1687\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1291\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1293\u001b[0m     x,\n\u001b[1;32m   1294\u001b[0m     y,\n\u001b[1;32m   1295\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1296\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1297\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1298\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1299\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1300\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1301\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1302\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1303\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1304\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1305\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39;49mpss_evaluation_shards,\n\u001b[1;32m   1306\u001b[0m )\n\u001b[1;32m   1308\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:353\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         flat_dataset \u001b[39m=\u001b[39m flat_dataset\u001b[39m.\u001b[39mshuffle(\u001b[39m1024\u001b[39m)\u001b[39m.\u001b[39mrepeat(epochs)\n\u001b[1;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m--> 353\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mflat_map(slice_batch_indices)\n\u001b[1;32m    355\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[1;32m    357\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:2313\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[0;34m(self, map_func, name)\u001b[0m\n\u001b[1;32m   2309\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2311\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2312\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m flat_map_op\n\u001b[0;32m-> 2313\u001b[0m \u001b[39mreturn\u001b[39;00m flat_map_op\u001b[39m.\u001b[39;49m_flat_map(\u001b[39mself\u001b[39;49m, map_func, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/data/ops/flat_map_op.py:24\u001b[0m, in \u001b[0;36m_flat_map\u001b[0;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flat_map\u001b[39m(input_dataset, map_func, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m   \u001b[39mreturn\u001b[39;00m _FlatMapDataset(input_dataset, map_func, name)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/data/ops/flat_map_op.py:33\u001b[0m, in \u001b[0;36m_FlatMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_dataset, map_func, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     32\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[0;32m---> 33\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m     34\u001b[0m       map_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(), dataset\u001b[39m=\u001b[39;49minput_dataset)\n\u001b[1;32m     35\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure, dataset_ops\u001b[39m.\u001b[39mDatasetSpec):\n\u001b[1;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_ops\u001b[39m.\u001b[39mget_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure)\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    259\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    262\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    266\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1222\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1221\u001b[0m   \u001b[39m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[0;32m-> 1222\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1223\u001b[0m   concrete\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:1192\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1191\u001b[0m     initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m-> 1192\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwargs, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m   1193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[1;32m   1195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m   1196\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m   \u001b[39m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    690\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    691\u001b[0m     tracing_compilation\u001b[39m.\u001b[39mScopeType\u001b[39m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    692\u001b[0m )\n\u001b[1;32m    693\u001b[0m \u001b[39m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mtrace_function(\n\u001b[1;32m    695\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[1;32m    696\u001b[0m )\n\u001b[1;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    699\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[39m=\u001b[39m tracing_options\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[39m=\u001b[39m _maybe_define_function(\n\u001b[1;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[1;32m    180\u001b[0m   )\n\u001b[1;32m    181\u001b[0m   _set_arg_keywords(concrete_function)\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tracing_options\u001b[39m.\u001b[39mbind_graph_to_function:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:284\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    283\u001b[0m   target_func_type \u001b[39m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 284\u001b[0m concrete_function \u001b[39m=\u001b[39m _create_concrete_function(\n\u001b[1;32m    285\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[1;32m    286\u001b[0m )\n\u001b[1;32m    288\u001b[0m \u001b[39mif\u001b[39;00m tracing_options\u001b[39m.\u001b[39mfunction_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m   tracing_options\u001b[39m.\u001b[39mfunction_cache\u001b[39m.\u001b[39madd(\n\u001b[1;32m    290\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    291\u001b[0m   )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:308\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[39mwith\u001b[39;00m func_graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m    304\u001b[0m   placeholder_bound_args \u001b[39m=\u001b[39m function_type\u001b[39m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    305\u001b[0m       placeholder_context\n\u001b[1;32m    306\u001b[0m   )\n\u001b[0;32m--> 308\u001b[0m traced_func_graph \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    309\u001b[0m     tracing_options\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    310\u001b[0m     tracing_options\u001b[39m.\u001b[39;49mpython_function,\n\u001b[1;32m    311\u001b[0m     placeholder_bound_args\u001b[39m.\u001b[39;49margs,\n\u001b[1;32m    312\u001b[0m     placeholder_bound_args\u001b[39m.\u001b[39;49mkwargs,\n\u001b[1;32m    313\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    314\u001b[0m     func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    315\u001b[0m     arg_names\u001b[39m=\u001b[39;49mfunction_type_utils\u001b[39m.\u001b[39;49mto_arg_names(function_type),\n\u001b[1;32m    316\u001b[0m     create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    317\u001b[0m )\n\u001b[1;32m    319\u001b[0m transform\u001b[39m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    321\u001b[0m graph_capture_container \u001b[39m=\u001b[39m traced_func_graph\u001b[39m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1061\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:597\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    594\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 597\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    598\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:231\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    232\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    233\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:161\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    160\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 161\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[1;32m    162\u001b[0m ret \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:335\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__.<locals>.slice_batch_indices\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m    333\u001b[0m num_in_full_batch \u001b[39m=\u001b[39m num_full_batches \u001b[39m*\u001b[39m batch_size\n\u001b[1;32m    334\u001b[0m first_k_indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mslice(indices, [\u001b[39m0\u001b[39m], [num_in_full_batch])\n\u001b[0;32m--> 335\u001b[0m first_k_indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mreshape(\n\u001b[1;32m    336\u001b[0m     first_k_indices, [num_full_batches, batch_size]\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m flat_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(first_k_indices)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_partial_batch_size:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1261\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py:210\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mreshape\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmanip.reshape\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     75\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m     76\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(tensor, shape, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=redefined-outer-name\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[39m  \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Reshapes a tensor.\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \n\u001b[1;32m     79\u001b[0m \u001b[39m  Given `tensor`, this operation returns a new `tf.Tensor` that has the same\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39m    A `Tensor`. Has the same type as `tensor`.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m   result \u001b[39m=\u001b[39m gen_array_ops\u001b[39m.\u001b[39;49mreshape(tensor, shape, name)\n\u001b[1;32m    211\u001b[0m   shape_util\u001b[39m.\u001b[39mmaybe_set_static_shape(result, shape)\n\u001b[1;32m    212\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/ops/gen_array_ops.py:8757\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(tensor, shape, name)\u001b[0m\n\u001b[1;32m   8755\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m   8756\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m-> 8757\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[1;32m   8758\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mReshape\u001b[39;49m\u001b[39m\"\u001b[39;49m, tensor\u001b[39m=\u001b[39;49mtensor, shape\u001b[39m=\u001b[39;49mshape, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   8759\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[1;32m   8760\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/op_def_library.py:778\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39mwith\u001b[39;00m g\u001b[39m.\u001b[39mas_default(), ops\u001b[39m.\u001b[39mname_scope(name) \u001b[39mas\u001b[39;00m scope:\n\u001b[1;32m    777\u001b[0m   \u001b[39mif\u001b[39;00m fallback:\n\u001b[0;32m--> 778\u001b[0m     _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n\u001b[1;32m    779\u001b[0m                            keywords, default_type_attr_map, attrs, inputs,\n\u001b[1;32m    780\u001b[0m                            input_types)\n\u001b[1;32m    781\u001b[0m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[1;32m    782\u001b[0m                            default_type_attr_map, attrs)\n\u001b[1;32m    783\u001b[0m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/op_def_library.py:531\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[0;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[1;32m    529\u001b[0m inferred \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 531\u001b[0m   inferred \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[1;32m    532\u001b[0m       values, name\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mis_ref)\n\u001b[1;32m    533\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    534\u001b[0m   \u001b[39m# When converting a python object such as a list of Dimensions, we\u001b[39;00m\n\u001b[1;32m    535\u001b[0m   \u001b[39m# need a dtype to be specified, thus tensor conversion may throw\u001b[39;00m\n\u001b[1;32m    536\u001b[0m   \u001b[39m# an exception which we will ignore and try again below.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:698\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[1;32m    697\u001b[0m preferred_dtype \u001b[39m=\u001b[39m preferred_dtype \u001b[39mor\u001b[39;00m dtype_hint\n\u001b[0;32m--> 698\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_conversion_registry\u001b[39m.\u001b[39;49mconvert(\n\u001b[1;32m    699\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[1;32m    700\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m    236\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:328\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    326\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    327\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 328\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:281\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 281\u001b[0m const_tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49m_create_graph_constant(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    282\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:270\u001b[0m, in \u001b[0;36m_create_graph_constant\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    268\u001b[0m dtype_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mtensor_value\u001b[39m.\u001b[39mtensor\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    269\u001b[0m attrs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: tensor_value, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype_value}\n\u001b[0;32m--> 270\u001b[0m const_tensor \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    271\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mConst\u001b[39;49m\u001b[39m\"\u001b[39;49m, [], [dtype_value\u001b[39m.\u001b[39;49mtype], attrs\u001b[39m=\u001b[39;49mattrs, name\u001b[39m=\u001b[39;49mname)\u001b[39m.\u001b[39moutputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m op_callbacks\u001b[39m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[1;32m    274\u001b[0m   \u001b[39m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[1;32m    275\u001b[0m   \u001b[39m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m   callback_outputs \u001b[39m=\u001b[39m op_callbacks\u001b[39m.\u001b[39minvoke_op_callbacks(\n\u001b[1;32m    277\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[39m=\u001b[39mname, graph\u001b[39m=\u001b[39mg)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    668\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[1;32m    669\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[0;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[1;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:2657\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   2654\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[1;32m   2655\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[1;32m   2656\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[0;32m-> 2657\u001b[0m   ret \u001b[39m=\u001b[39m Operation\u001b[39m.\u001b[39;49mfrom_node_def(\n\u001b[1;32m   2658\u001b[0m       node_def,\n\u001b[1;32m   2659\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2660\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m   2661\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[1;32m   2662\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[1;32m   2663\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m   2664\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[1;32m   2665\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def,\n\u001b[1;32m   2666\u001b[0m   )\n\u001b[1;32m   2667\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[1;32m   2668\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tensorflow/python/framework/ops.py:1112\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[0;34m(cls, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(node_def, node_def_pb2\u001b[39m.\u001b[39mNodeDef):\n\u001b[1;32m   1110\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArgument node_def must be a NodeDef. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1111\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived an instance of type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(node_def)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1112\u001b[0m \u001b[39mif\u001b[39;00m node_def\u001b[39m.\u001b[39;49mByteSize() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m<<\u001b[39m \u001b[39m31\u001b[39m) \u001b[39mor\u001b[39;00m node_def\u001b[39m.\u001b[39mByteSize() \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1113\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1114\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot create a tensor proto whose content is larger than 2GB. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1115\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSize of tensor is \u001b[39m\u001b[39m{\u001b[39;00mnode_def\u001b[39m.\u001b[39mByteSize()\u001b[39m}\u001b[39;00m\u001b[39m bytes.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1117\u001b[0m \u001b[39m# TODO(mdan): This does not belong here. Graph::AddNode should handle it.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:1065\u001b[0m, in \u001b[0;36m_AddByteSizeMethod.<locals>.ByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m   \u001b[39mfor\u001b[39;00m field_descriptor, field_value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mListFields():\n\u001b[0;32m-> 1065\u001b[0m     size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m field_descriptor\u001b[39m.\u001b[39;49m_sizer(field_value)\n\u001b[1;32m   1066\u001b[0m   \u001b[39mfor\u001b[39;00m tag_bytes, value_bytes \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unknown_fields:\n\u001b[1;32m   1067\u001b[0m     size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tag_bytes) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(value_bytes)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/google/protobuf/internal/encoder.py:361\u001b[0m, in \u001b[0;36mMapSizer.<locals>.FieldSize\u001b[0;34m(map_value)\u001b[0m\n\u001b[1;32m    359\u001b[0m   total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m message_sizer(entry_msg)\n\u001b[1;32m    360\u001b[0m   \u001b[39mif\u001b[39;00m is_message_map:\n\u001b[0;32m--> 361\u001b[0m     value\u001b[39m.\u001b[39;49mByteSize()\n\u001b[1;32m    362\u001b[0m \u001b[39mreturn\u001b[39;00m total\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:1065\u001b[0m, in \u001b[0;36m_AddByteSizeMethod.<locals>.ByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m   \u001b[39mfor\u001b[39;00m field_descriptor, field_value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mListFields():\n\u001b[0;32m-> 1065\u001b[0m     size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m field_descriptor\u001b[39m.\u001b[39;49m_sizer(field_value)\n\u001b[1;32m   1066\u001b[0m   \u001b[39mfor\u001b[39;00m tag_bytes, value_bytes \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unknown_fields:\n\u001b[1;32m   1067\u001b[0m     size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tag_bytes) \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(value_bytes)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/google/protobuf/internal/encoder.py:306\u001b[0m, in \u001b[0;36mMessageSizer.<locals>.FieldSize\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mFieldSize\u001b[39m(value):\n\u001b[0;32m--> 306\u001b[0m   l \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39;49mByteSize()\n\u001b[1;32m    307\u001b[0m   \u001b[39mreturn\u001b[39;00m tag_size \u001b[39m+\u001b[39m local_VarintSize(l) \u001b[39m+\u001b[39m l\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:1064\u001b[0m, in \u001b[0;36m_AddByteSizeMethod.<locals>.ByteSize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1062\u001b[0m   size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m descriptor\u001b[39m.\u001b[39mfields_by_name[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39m_sizer(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue)\n\u001b[1;32m   1063\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m   \u001b[39mfor\u001b[39;00m field_descriptor, field_value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mListFields():\n\u001b[1;32m   1065\u001b[0m     size \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m field_descriptor\u001b[39m.\u001b[39m_sizer(field_value)\n\u001b[1;32m   1066\u001b[0m   \u001b[39mfor\u001b[39;00m tag_bytes, value_bytes \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unknown_fields:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:822\u001b[0m, in \u001b[0;36m_AddListFieldsMethod.<locals>.ListFields\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mListFields\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 822\u001b[0m   all_fields \u001b[39m=\u001b[39m [item \u001b[39mfor\u001b[39;49;00m item \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fields\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m _IsPresent(item)]\n\u001b[1;32m    823\u001b[0m   all_fields\u001b[39m.\u001b[39msort(key \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m item: item[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumber)\n\u001b[1;32m    824\u001b[0m   \u001b[39mreturn\u001b[39;00m all_fields\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:822\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mListFields\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 822\u001b[0m   all_fields \u001b[39m=\u001b[39m [item \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fields\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m _IsPresent(item)]\n\u001b[1;32m    823\u001b[0m   all_fields\u001b[39m.\u001b[39msort(key \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m item: item[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumber)\n\u001b[1;32m    824\u001b[0m   \u001b[39mreturn\u001b[39;00m all_fields\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/google/protobuf/internal/python_message.py:806\u001b[0m, in \u001b[0;36m_IsPresent\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[39mreturn\u001b[39;00m message\n\u001b[1;32m    803\u001b[0m   \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mFromString \u001b[39m=\u001b[39m \u001b[39mstaticmethod\u001b[39m(FromString)\n\u001b[0;32m--> 806\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_IsPresent\u001b[39m(item):\n\u001b[1;32m    807\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Given a (FieldDescriptor, value) tuple from _fields, return true if the\u001b[39;00m\n\u001b[1;32m    808\u001b[0m \u001b[39m  value should be included in the list returned by ListFields().\"\"\"\u001b[39;00m\n\u001b[1;32m    810\u001b[0m   \u001b[39mif\u001b[39;00m item[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlabel \u001b[39m==\u001b[39m _FieldDescriptor\u001b[39m.\u001b[39mLABEL_REPEATED:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "face_detector = MediaPipeFaceDetector()\n",
    "qualifier = DeepfaceFaceQualifier()\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    faces = face_detector.detect(frame)\n",
    "    qualifications = [qualifier.qualify(face) for face in faces]\n",
    "    display = OpenCVFaceQualificationDisplay(qualifications, frame)\n",
    "    display.show()\n",
    "\n",
    "    cv.imshow('Video', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACE AUTHENTICATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthenticationScreen(ABC):\n",
    "    @abstractmethod\n",
    "    def display(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    next_state : str\n",
    "    actions: List[Callable[[], bool]]\n",
    "\n",
    "    @staticmethod\n",
    "    def to(next_state : str) -> 'Transition':\n",
    "        return Transition(next_state, [])\n",
    "\n",
    "    def when(self, action : Callable[[], bool]) -> 'Transition':\n",
    "        self.actions.append(action)\n",
    "        return self\n",
    "\n",
    "    def evaluate_transition(self) -> bool:\n",
    "        for action in self.actions:\n",
    "            if action():\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "@dataclass\n",
    "class State:\n",
    "    is_initial : bool\n",
    "    name : str\n",
    "    screen : Callable[[np.ndarray], AuthenticationScreen]\n",
    "    transitions : List[Transition]\n",
    "    on_enter : Callable[[], None] = lambda : None\n",
    "    on_exit : Callable[[], None] = lambda : None\n",
    "\n",
    "    @staticmethod\n",
    "    def default(name : str, screen : Callable[[np.ndarray], AuthenticationScreen]) -> 'State':\n",
    "        return State(True, name, screen, [])\n",
    "    \n",
    "    @staticmethod\n",
    "    def of(name : str, screen : Callable[[np.ndarray], AuthenticationScreen]) -> 'State':\n",
    "        return State(False, name, screen, [])\n",
    "\n",
    "    def do(self, transition : Transition) -> 'State':\n",
    "        self.transitions.append(transition)\n",
    "        return self\n",
    "    \n",
    "    def do_on_enter(self, action : Callable[[], None]) -> 'State':\n",
    "        self.on_enter = action\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class StateMachine(ABC):\n",
    "    class Builder:\n",
    "        def __init__(self) -> None:\n",
    "            self.states = []\n",
    "        \n",
    "        def add_state(self, state : State) -> 'Builder':\n",
    "            self.states.append(state)\n",
    "            return self\n",
    "        \n",
    "        def build(self) -> 'StateMachine':\n",
    "            return StateMachine(self.states)\n",
    "    \n",
    "    @staticmethod\n",
    "    def start_building() -> Builder:\n",
    "        return StateMachine.Builder()\n",
    "\n",
    "    def __init__(self, states: List[State] ) -> None:\n",
    "        self.states = states\n",
    "        self.current_state = list(filter(lambda x: x.is_initial, states))[0]\n",
    "        self.current_state.on_enter()\n",
    "    \n",
    "    def execute(self, frame : np.array):\n",
    "        self.current_state.screen(frame).display()\n",
    "        self.evaluate_conditions()\n",
    "    \n",
    "    def evaluate_conditions(self) -> None:\n",
    "        for transition in self.current_state.transitions:\n",
    "            if transition.evaluate_transition():\n",
    "                self.current_state.on_exit()\n",
    "                self.current_state = list(filter(lambda x: x.name == transition.next_state, self.states))[0]\n",
    "                self.current_state.on_enter()\n",
    "                break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFRAESTRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IconAuthenticationScreen(AuthenticationScreen):\n",
    "    def __init__(self, frame : np.ndarray, icon_path : str, color : tuple, text : str, bg_color : str = None, alpha : float = 0.15) -> None:\n",
    "        self.frame = frame\n",
    "        self.icon_path = icon_path\n",
    "        self.color = color\n",
    "        self.text = text\n",
    "        self.bg_color = self.color if(bg_color == None) else bg_color\n",
    "        self.alpha  = alpha\n",
    "\n",
    "    def overlay_green_screen(self):\n",
    "        # Create a green screen of the same size as the frame\n",
    "        screen = np.full(self.frame.shape, self.bg_color, dtype=np.uint8)\n",
    "        alpha = self.alpha\n",
    "        self.frame[:,:] = cv2.addWeighted(self.frame, 1 - alpha, screen, alpha, 0)\n",
    "\n",
    "    def display(self) -> None:\n",
    "        self.overlay_green_screen()\n",
    "        ImageUtils.overlay_icon(image=self.frame, \n",
    "                                icon_path=self.icon_path, \n",
    "                                color=self.color, \n",
    "                                icon_size=200, \n",
    "                                point=(self.frame.shape[1] // 2, self.frame.shape[0] // 2))\n",
    "        #draw text that says Press any key to unlock\n",
    "        font_weight=2\n",
    "        font_size=1.2\n",
    "        text = self.text\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "        text_width, text_height = text_size\n",
    "        text_x = self.frame.shape[1] // 2 - text_width // 2\n",
    "        text_y = self.frame.shape[0] // 2 + text_height + 150\n",
    "        cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, self.color, font_weight, lineType = cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LockAuthenticationScreen(IconAuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray) -> None:\n",
    "        super().__init__(frame, \"./assets/lock.png\", (255, 255, 255), \"PRESS A KEY TO START FACE RECOGNITION\", bg_color=(0, 0, 0), alpha=0.4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessGrantedAuthenticationScreen(IconAuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray) -> None:\n",
    "        super().__init__(frame, \"./assets/success.png\", (0, 255, 0), \"ACCESS GRANTED\", bg_color=(0, 255, 0), alpha = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessDeniedAuthenticationScreen(IconAuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray) -> None:\n",
    "        super().__init__(frame, \"./assets/danger.png\", (0, 0, 255), \"ACCESS DENIED\", bg_color=(0, 0, 255), alpha = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "class FaceRecognizerAuthenticationScreen(AuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray ) -> None:\n",
    "        self.frame = frame\n",
    "\n",
    "    def display(self) -> None:\n",
    "        self.draw_landmarks()\n",
    "        self.draw_processing_text()\n",
    "    \n",
    "    def draw_processing_text(self):\n",
    "        font_weight=2\n",
    "        font_size=1.2\n",
    "        text = \"PROCESSING FACE\"\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "        text_width, text_height = text_size\n",
    "        text_x = self.frame.shape[1] // 2 - text_width // 2\n",
    "        text_y = 300 \n",
    "        cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, (255, 255, 255), font_weight, lineType = cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    def draw_landmarks(self):\n",
    "        base_options = python.BaseOptions(model_asset_path='./face_landmarker.task')\n",
    "        options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                            output_face_blendshapes=True,\n",
    "                                            output_facial_transformation_matrixes=True)\n",
    "        detector = vision.FaceLandmarker.create_from_options(options)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=self.frame)\n",
    "        detection_result = detector.detect(mp_image)\n",
    "        self.frame[::] = self.draw_landmarks_on_image(self.frame, detection_result)\n",
    "    \n",
    "    def draw_landmarks_on_image(self, rgb_image, detection_result):\n",
    "        face_landmarks_list = detection_result.face_landmarks\n",
    "        annotated_image = np.copy(rgb_image)\n",
    "\n",
    "        # Loop through the detected faces to visualize.\n",
    "        for idx in range(len(face_landmarks_list)):\n",
    "            face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "            # Draw the face landmarks.\n",
    "            face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "            face_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "            ])\n",
    "\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles\n",
    "                .get_default_face_mesh_tesselation_style())\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles\n",
    "                .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "        return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputManager:\n",
    "    @staticmethod\n",
    "    def is_space_pressed() -> bool:\n",
    "        return cv.waitKey(1) & 0xFF == ord(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATE MACHINE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701815629.907596       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815629.911140       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815630.102779       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815630.103279       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815630.235348       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815630.235952       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815630.369278       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815630.369739       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815630.502810       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815630.503308       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815630.634874       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815630.635374       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815630.768743       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815630.769292       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815630.901973       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815630.902464       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815631.036673       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815631.037096       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815631.170661       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815631.171087       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815631.300484       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815631.300904       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815631.435628       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815631.436124       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815631.568068       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815631.568583       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815631.702161       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815631.702672       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815631.835336       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815631.835874       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815632.001876       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815632.002370       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815632.135189       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815632.135691       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815632.267940       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815632.268438       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815632.401192       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815632.401727       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815632.536618       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815632.537113       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815632.666727       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815632.667155       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815632.801164       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815632.801666       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815632.934099       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815632.934589       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815633.068013       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815633.068512       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815633.201026       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815633.201496       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815633.334639       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815633.335133       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815633.467934       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815633.468431       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815633.600602       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815633.601118       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815633.732792       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815633.733214       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815633.867158       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815633.867715       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815634.000953       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815634.001438       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815634.134213       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815634.134712       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815634.267576       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815634.268086       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815634.401138       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815634.401625       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815634.533727       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815634.534145       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701815634.667038       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701815634.667531       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 31\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m state_machine\u001b[39m.\u001b[39mexecute(frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m cv\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mVideo\u001b[39m\u001b[39m'\u001b[39m, frame)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mif\u001b[39;00m cv\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m1\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import threading\n",
    "import random\n",
    "\n",
    "#exist a timer\n",
    "is_authorizated = False\n",
    "finished_recognition = False\n",
    "\n",
    "def set_authorizated():\n",
    "    global is_authorizated\n",
    "    global finished_recognition\n",
    "    is_authorizated = random.choice([True, False])\n",
    "    finished_recognition=True\n",
    "\n",
    "\n",
    "timer = threading.Timer(5.0, lambda: set_authorizated())\n",
    "\n",
    "def is_access_granted() -> bool:\n",
    "    global is_authorizated\n",
    "    global finished_recognition\n",
    "    return is_authorizated and finished_recognition\n",
    "\n",
    "def is_access_denied() -> bool:\n",
    "    global is_authorizated\n",
    "    global finished_recognition\n",
    "    return not is_authorizated and finished_recognition\n",
    "\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "\n",
    "state_machine = StateMachine.start_building().add_state(\n",
    "        State.default(\n",
    "            name=\"LOCK\", \n",
    "            screen=LockAuthenticationScreen)\n",
    "                    .do(Transition.to(\"RECOGNIZING\").when(InputManager.is_space_pressed))\n",
    "    ).add_state(\n",
    "        State.of( \n",
    "            name=\"RECOGNIZING\", \n",
    "            screen=FaceRecognizerAuthenticationScreen)\n",
    "                .do(Transition.to(\"GRANTED\").when(is_access_granted))\n",
    "                .do_on_enter(timer.start)\n",
    "    ).add_state(\n",
    "        State.of(\n",
    "            name=\"GRANTED\", \n",
    "            screen =AccessGrantedAuthenticationScreen)\n",
    "                    .do(Transition.to(\"LOCK\").when(InputManager.is_space_pressed))\n",
    "    ).add_state(\n",
    "\n",
    "    State.of(\n",
    "          name=\"DENIED\", \n",
    "          screen=AccessDeniedAuthenticationScreen)\n",
    "            .do(Transition.to(\"LOCK\").when(InputManager.is_space_pressed))\n",
    "    ).build()\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    state_machine.execute(frame)\n",
    "    cv.imshow('Video', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
