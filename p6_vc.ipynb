{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from retinaface import RetinaFace\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Emotions:\n",
    "    name : str\n",
    "    quantity : int\n",
    "\n",
    "@dataclass\n",
    "class Point:\n",
    "    x : float\n",
    "    y : float\n",
    "\n",
    "@dataclass\n",
    "class BoundingBox:\n",
    "    origin : Point\n",
    "    end: Point\n",
    "\n",
    "\n",
    "class Genre:\n",
    "    male = 'MALE'\n",
    "    female = 'FEMALE'\n",
    "\n",
    "    def __init__(self, genre : str):\n",
    "        self.genre = genre\n",
    "\n",
    "    @staticmethod\n",
    "    def female():\n",
    "        return Genre(Genre.female)\n",
    "    \n",
    "    @staticmethod\n",
    "    def male():\n",
    "        return Genre(Genre.male)\n",
    "    \n",
    "    def isMale(self):\n",
    "        return self.genre == Genre.male\n",
    "\n",
    "    def isFemale(self):\n",
    "        return self.genre == Genre.female\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    age : str\n",
    "    genre : Genre\n",
    "    emotions : List[Emotions]\n",
    "    bounding_box : BoundingBox\n",
    "    image : np.ndarray\n",
    "\n",
    "@dataclass\n",
    "class FaceDetectorResult:\n",
    "    image : np.ndarray \n",
    "    bounding_box : BoundingBox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List\n",
    "\n",
    "class FaceDetector(ABC):\n",
    "    @abstractmethod\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        pass\n",
    "\n",
    "class FaceQualifier(ABC):\n",
    "    @abstractmethod\n",
    "    def qualify(self, face_detector_result : FaceDetectorResult) -> Person:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infraestructura\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageUtils:\n",
    "    @staticmethod\n",
    "    def crop(image : np.array, bounding_box : BoundingBox) -> np.ndarray:\n",
    "        return image[bounding_box.origin.x:bounding_box.end.x, bounding_box.origin.y:bounding_box.end.y]\n",
    "    \n",
    "    @staticmethod\n",
    "    def overlay_icon(image: np.array, icon_path: str, color: tuple, icon_size : int, point : tuple) -> np.ndarray:\n",
    "        icon = cv2.imread(icon_path, cv2.IMREAD_GRAYSCALE)\n",
    "        icon = cv2.resize(icon, (icon_size, icon_size))\n",
    "        mask = icon == 0\n",
    "        color_layer = np.full((icon.shape[0], icon.shape[1], 3), color, dtype=np.uint8)\n",
    "        np.copyto(image[point[1]-icon.shape[0]//2:point[1]+icon.shape[0]//2, point[0]-icon.shape[1]//2:point[0]+icon.shape[1]//2], color_layer, where=mask[:,:,None])\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ViolaJonesFaceDetector(FaceDetector):\n",
    "    cascPathface = os.path.dirname(\n",
    "        cv2.__file__) + \"/data/haarcascade_frontalface_alt2.xml\"\n",
    "\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        faceCascade = cv2.CascadeClassifier(self.cascPathface)\n",
    "        gray = self._convert_image_to_gray(image)\n",
    "        faces = faceCascade.detectMultiScale(gray,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=5,\n",
    "                                         minSize=(60, 60),\n",
    "                                         flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        return self._convert_to_face_detector_result(image, faces)\n",
    "        \n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x+w, y+h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    \n",
    "    def _convert_image_to_gray(self, image : np.ndarray) -> np.ndarray:\n",
    "        import cv2\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "class RetinafaceFaceDetector(FaceDetector):\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        faces = RetinaFace.detect_faces(image);\n",
    "        return self._convert_to_face_detector_result(image, [faces[key]['facial_area'] for key in faces.keys()])\n",
    "    \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(w, h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "\n",
    "class MediaPipeFaceDetector(FaceDetector):\n",
    "    def detect(self, image : np.ndarray) -> List[FaceDetectorResult]:\n",
    "        BaseOptions = mp.tasks.BaseOptions\n",
    "        FaceDetector = mp.tasks.vision.FaceDetector\n",
    "        FaceDetectorOptions = mp.tasks.vision.FaceDetectorOptions\n",
    "        VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "        options = FaceDetectorOptions(\n",
    "            base_options=BaseOptions(model_asset_path='./blaze_face_short_range.tflite'),\n",
    "            running_mode=VisionRunningMode.IMAGE)\n",
    "\n",
    "        with FaceDetector.create_from_options(options) as detector:\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n",
    "            face_detector_result = detector.detect(mp_image)\n",
    "            return self._convert_to_face_detector_result(image, [(x.bounding_box.origin_x, x.bounding_box.origin_y, x.bounding_box.width, x.bounding_box.height) for x in face_detector_result.detections])\n",
    "        \n",
    "    def _convert_to_face_detector_result(self, image, faces) -> List[FaceDetectorResult]:\n",
    "        bounding_boxes = [BoundingBox(Point(x, y), Point(x + w, y + h)) for (x, y, w, h) in faces]\n",
    "        return [FaceDetectorResult(ImageUtils.crop(image, bounding_box), bounding_box) for bounding_box in bounding_boxes]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockFaceQualifier(FaceQualifier):\n",
    "    def qualify(self, face_detector_result):\n",
    "        return Person(\n",
    "            age=43, \n",
    "            genre=Genre.male(), \n",
    "            emotions=[Emotions('happy', 0.8),\n",
    "                      Emotions('sad', 0.5),\n",
    "                      Emotions('angry', 0.3)\n",
    "                      ], \n",
    "            bounding_box=face_detector_result.bounding_box, \n",
    "            image=face_detector_result.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "class DeepfaceFaceQualifier(FaceQualifier):\n",
    "    def qualify(self, face_detector_result: FaceDetectorResult) -> Person:\n",
    "        face_image = face_detector_result.image\n",
    "        deepface_result = DeepFace.analyze(img_path = face_image, actions = ['age', 'gender', 'emotion'], enforce_detection=False)[0]\n",
    "        \n",
    "        age = str(deepface_result[\"age\"])\n",
    "        gender = Genre( \"MALE\" if deepface_result[\"dominant_gender\"] == \"Man\" else \"FEMALE\")\n",
    "        emotion_predictions = deepface_result[\"emotion\"]\n",
    "        emotions = [Emotions(name=emotion, quantity=round(probability, 3)) for emotion, probability in emotion_predictions.items()]\n",
    "        sorted_emotions = sorted(emotions, key=lambda x: x.quantity, reverse=True)\n",
    "\n",
    "        person = Person(\n",
    "            age = age,\n",
    "            genre = gender,\n",
    "            emotions = sorted_emotions[0:3],\n",
    "            bounding_box = face_detector_result.bounding_box,\n",
    "            image = face_image\n",
    "        )\n",
    "        \n",
    "        return person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualificationDisplay:\n",
    "    def show():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "class OpenCVFaceQualificationDisplay(FaceQualificationDisplay):\n",
    "    def __init__(self, people : List[Person], frame : np.ndarray) -> None:\n",
    "        self.people = people\n",
    "        self.frame = frame\n",
    "    \n",
    "    def display_emotions_right_of_person(self, person: Person, padding_x: int, padding_y: int) -> None:\n",
    "        width = 200\n",
    "        border_width =2\n",
    "        rectangle_origin = (person.bounding_box.end.x + border_width, person.bounding_box.origin.y - border_width)\n",
    "        rectangle_end = (rectangle_origin[0] + width, person.bounding_box.end.y + border_width)\n",
    "        if rectangle_end[0] > self.frame.shape[1]:\n",
    "            rectangle_end = (self.frame.shape[1], rectangle_end[1])\n",
    "        if rectangle_end[1] > self.frame.shape[0]:\n",
    "            rectangle_end = (rectangle_end[0], self.frame.shape[0])\n",
    "        #blur the rectangle background\n",
    "        self.frame[rectangle_origin[1]:rectangle_end[1], rectangle_origin[0]:rectangle_end[0]] = cv.GaussianBlur(self.frame[rectangle_origin[1]:rectangle_end[1], rectangle_origin[0]:rectangle_end[0]], (75, 75), 0) / 1.5\n",
    "\n",
    "        font_weight=1\n",
    "        font_size=0.8\n",
    "        text = \"Emotions\"\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "        text_width, text_height = text_size\n",
    "        text_x = rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - text_width // 2\n",
    "        text_y = rectangle_origin[1] + text_height + padding_y // 2\n",
    "        cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, (255, 255, 255), font_weight, lineType = cv2.LINE_AA)\n",
    "\n",
    "        gap = 40\n",
    "\n",
    "        for x in person.emotions:\n",
    "            #draw text of emotion\n",
    "            font_weight=1\n",
    "            font_size=0.5\n",
    "            text = x.name + \": \" + str(x.quantity)\n",
    "            text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "            text_width, text_height = text_size\n",
    "            text_x = rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - text_width // 2\n",
    "            text_y = (rectangle_origin[1] + text_height + padding_y // 2) + gap\n",
    "            cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, (255, 255, 255), font_weight, lineType = cv2.LINE_AA)\n",
    "            gap+=30\n",
    "\n",
    "\n",
    "    def display_age_on_top_of_person(self, person: Person, padding_x: int, padding_y: int) -> None:\n",
    "        gap = 26\n",
    "        font_thinkness = 2\n",
    "        icon_size = 38\n",
    "        triangle_size = 20\n",
    "\n",
    "        text = str(person.age)\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, 1.5, font_thinkness)\n",
    "        text_width, text_height = text_size\n",
    "\n",
    "        rectangle_origin = ((person.bounding_box.origin.x + (person.bounding_box.end.x - person.bounding_box.origin.x) // 2 - (text_width + padding_x) // 2), person.bounding_box.origin.y - text_height - padding_y - 50)\n",
    "        rectangle_end = (rectangle_origin[0] + text_width + padding_x, rectangle_origin[1] + text_height + padding_y)\n",
    "\n",
    "        # Draw the rectangle with a pointer\n",
    "        cv.rectangle(self.frame, rectangle_origin, rectangle_end, (67, 193, 246), cv.FILLED)\n",
    "\n",
    "        # Center the text within the rectangle\n",
    "        text_x = rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - text_width // 2\n",
    "        text_y = rectangle_origin[1] + text_height + padding_y // 2\n",
    "\n",
    "        # Draw the text\n",
    "        cv.putText(self.frame, text, (text_x - gap, text_y), cv.FONT_HERSHEY_DUPLEX, 1.5, (47, 123, 222), font_thinkness, lineType = cv2.LINE_AA)\n",
    "\n",
    "        triangle = np.array([[rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 - triangle_size + 5, rectangle_end[1]],\n",
    "                            [rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2 + triangle_size - 5, rectangle_end[1]],\n",
    "                            [rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2, rectangle_end[1] + triangle_size] ], np.int32)\n",
    "        \n",
    "        cv.drawContours(self.frame, [triangle], 0, (67, 193, 246), cv.FILLED)\n",
    "\n",
    "        male_icon = cv.imread(\"./assets/male.png\" if person.genre.isMale() else \"./assets/female.png\")  # Correct the path to your male icon image\n",
    "        if male_icon is not None:\n",
    "            male_icon = cv.resize(male_icon, (icon_size, icon_size))\n",
    "            rectangle_center = (rectangle_origin[0] + (rectangle_end[0] - rectangle_origin[0]) // 2, rectangle_origin[1] + icon_size // 2 + padding_y//2)\n",
    "            self.overlay_icon(male_icon, (rectangle_center[0] + gap, rectangle_center[1]))\n",
    "        else:\n",
    "            print(\"Male icon not found or there's an error in reading the image.\")\n",
    "    \n",
    "    def overlay_icon(self, icon, center):\n",
    "        \"\"\"Overlay an icon image at the specified top left corner position.\"\"\"\n",
    "        h, w, _ = icon.shape\n",
    "        x, y = center\n",
    "        # Check if the coordinates are within the frame bounds\n",
    "        #draw only if the icon fits inside the frame\n",
    "        if x-w//2 >= 0 and y-h//2 >= 0 and x+w//2 < self.frame.shape[1] and y+h//2 < self.frame.shape[0]:\n",
    "            self.frame[y-h//2:y+h//2, x-w//2:x+w//2] = icon\n",
    "    \n",
    "    def display_box_around_person(self, person : Person) -> None:\n",
    "        cv.rectangle(self.frame, (person.bounding_box.origin.x, \n",
    "                                  person.bounding_box.origin.y), \n",
    "                                  (person.bounding_box.end.x, \n",
    "                                   person.bounding_box.end.y), \n",
    "                                   (67, 193, 246), 2)\n",
    "    \n",
    "    def show(self):\n",
    "        for person in self.people:\n",
    "            self.display_box_around_person(person)\n",
    "            self.display_age_on_top_of_person(person, 130, 38)\n",
    "            self.display_emotions_right_of_person(person, 130, 38)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MockFaceComparator(FaceComparator):\n",
    "    def qualify(self, first_face, second_face):\n",
    "        return FaceComparatorResult(0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfaceFaceComparator(FaceComparator):\n",
    "    def qualify(self, first_face, second_face):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV2 INTERFACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceQualificationDisplay(ABC):\n",
    "    @abstractmethod\n",
    "    def show(self) -> None:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701817227.947278       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: age:   0%|          | 0/3 [00:00<?, ?it/s]    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 3/3 [00:01<00:00,  2.03it/s]\n",
      "I0000 00:00:1701817229.514139       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  4.74it/s]\n",
      "I0000 00:00:1701817230.244113       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  5.79it/s]\n",
      "I0000 00:00:1701817230.843925       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.62it/s]\n",
      "I0000 00:00:1701817231.377019       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  7.16it/s]\n",
      "I0000 00:00:1701817231.875569       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.39it/s]\n",
      "I0000 00:00:1701817232.409534       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701817232.476348       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  5.85it/s]\n",
      "I0000 00:00:1701817233.077065       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.01it/s]\n",
      "I0000 00:00:1701817233.675301       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701817233.742810       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.25it/s]\n",
      "I0000 00:00:1701817234.309083       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.16it/s]\n",
      "I0000 00:00:1701817234.876203       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  5.58it/s]\n",
      "I0000 00:00:1701817235.508595       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  5.93it/s]\n",
      "I0000 00:00:1701817236.109047       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  5.98it/s]\n",
      "I0000 00:00:1701817236.674308       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.10it/s]\n",
      "I0000 00:00:1701817237.242048       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.29it/s]\n",
      "I0000 00:00:1701817237.807226       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "I0000 00:00:1701817237.874462       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.26it/s]\n",
      "I0000 00:00:1701817238.441624       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.68it/s]\n",
      "I0000 00:00:1701817238.974764       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.62it/s]\n",
      "I0000 00:00:1701817239.506850       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  7.17it/s]\n",
      "I0000 00:00:1701817240.007146       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  7.11it/s]\n",
      "I0000 00:00:1701817240.506745       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.80it/s]\n",
      "I0000 00:00:1701817241.040372       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.73it/s]\n",
      "I0000 00:00:1701817241.572251       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n",
      "I0000 00:00:1701817242.206454       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "Action: emotion: 100%|██████████| 3/3 [00:00<00:00,  6.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "face_detector = MediaPipeFaceDetector()\n",
    "qualifier = DeepfaceFaceQualifier()\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    faces = face_detector.detect(frame)\n",
    "    qualifications = [qualifier.qualify(face) for face in faces]\n",
    "    display = OpenCVFaceQualificationDisplay(qualifications, frame)\n",
    "    display.show()\n",
    "\n",
    "    cv.imshow('Video', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACE AUTHENTICATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Transition:\n",
    "    next_state : str\n",
    "    actions: List[Callable[[], bool]]\n",
    "\n",
    "    @staticmethod\n",
    "    def to(next_state : str) -> 'Transition':\n",
    "        return Transition(next_state, [])\n",
    "\n",
    "    def when(self, action : Callable[[], bool]) -> 'Transition':\n",
    "        self.actions.append(action)\n",
    "        return self\n",
    "\n",
    "    def evaluate_transition(self) -> bool:\n",
    "        for action in self.actions:\n",
    "            if action():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "class AuthenticationScreen(ABC):\n",
    "    @abstractmethod\n",
    "    def display(self) -> None:\n",
    "        pass\n",
    "    \n",
    "@dataclass\n",
    "class State:\n",
    "    is_initial : bool\n",
    "    name : str\n",
    "    screen : Callable[[], AuthenticationScreen]\n",
    "    transitions : List[Transition]\n",
    "    on_enter : Callable[[np.ndarray], None] = lambda frame: None\n",
    "    on_exit : Callable[[np.ndarray], None] = lambda frame: None\n",
    "\n",
    "    @staticmethod\n",
    "    def default(name : str, screen : Callable[[np.ndarray], AuthenticationScreen]) -> 'State':\n",
    "        return State(True, name, screen, [])\n",
    "    \n",
    "    @staticmethod\n",
    "    def of(name : str, screen : Callable[[np.ndarray], AuthenticationScreen]) -> 'State':\n",
    "        return State(False, name, screen, [])\n",
    "\n",
    "    def do(self, transition : Transition) -> 'State':\n",
    "        self.transitions.append(transition)\n",
    "        return self\n",
    "    \n",
    "    def do_on_enter(self, action : Callable[[], None]) -> 'State':\n",
    "        self.on_enter = action\n",
    "        return self\n",
    "\n",
    "@dataclass\n",
    "class FaceComparatorResult:\n",
    "    similarity: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "class StateMachine(ABC):\n",
    "    class Builder:\n",
    "        def __init__(self) -> None:\n",
    "            self.states = []\n",
    "        \n",
    "        def add_state(self, state : State) -> 'Builder':\n",
    "            self.states.append(state)\n",
    "            return self\n",
    "        \n",
    "        def build(self) -> 'StateMachine':\n",
    "            return StateMachine(self.states)\n",
    "    \n",
    "    @staticmethod\n",
    "    def start_building() -> Builder:\n",
    "        return StateMachine.Builder()\n",
    "\n",
    "    def __init__(self, states: List[State] ) -> None:\n",
    "        self.states = states\n",
    "        self.current_state = list(filter(lambda x: x.is_initial, states))[0]\n",
    "        self.current_state.on_enter(frame)\n",
    "    \n",
    "    def execute(self, frame : np.array):\n",
    "        self.current_state.screen(frame).display()\n",
    "        self.evaluate_conditions(frame)\n",
    "    \n",
    "    def evaluate_conditions(self, frame) -> None:\n",
    "        for transition in self.current_state.transitions:\n",
    "            if transition.evaluate_transition():\n",
    "                self.current_state.on_exit(frame)\n",
    "                self.current_state = list(filter(lambda x: x.name == transition.next_state, self.states))[0]\n",
    "                self.current_state.on_enter(frame)\n",
    "                break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFRAESTRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FaceComparator:\n",
    "    def compare(self) -> FaceComparatorResult:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IconAuthenticationScreen(AuthenticationScreen):\n",
    "    def __init__(self, frame : np.ndarray, icon_path : str, color : tuple, text : str, bg_color : str = None, alpha : float = 0.15) -> None:\n",
    "        self.frame = frame\n",
    "        self.icon_path = icon_path\n",
    "        self.color = color\n",
    "        self.text = text\n",
    "        self.bg_color = self.color if(bg_color == None) else bg_color\n",
    "        self.alpha  = alpha\n",
    "\n",
    "    def overlay_green_screen(self):\n",
    "        # Create a green screen of the same size as the frame\n",
    "        screen = np.full(self.frame.shape, self.bg_color, dtype=np.uint8)\n",
    "        alpha = self.alpha\n",
    "        self.frame[:,:] = cv2.addWeighted(self.frame, 1 - alpha, screen, alpha, 0)\n",
    "\n",
    "    def display(self) -> None:\n",
    "        self.overlay_green_screen()\n",
    "        ImageUtils.overlay_icon(image=self.frame, \n",
    "                                icon_path=self.icon_path, \n",
    "                                color=self.color, \n",
    "                                icon_size=200, \n",
    "                                point=(self.frame.shape[1] // 2, self.frame.shape[0] // 2))\n",
    "        #draw text that says Press any key to unlock\n",
    "        font_weight=2\n",
    "        font_size=1.2\n",
    "        text = self.text\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "        text_width, text_height = text_size\n",
    "        text_x = self.frame.shape[1] // 2 - text_width // 2\n",
    "        text_y = self.frame.shape[0] // 2 + text_height + 150\n",
    "        cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, self.color, font_weight, lineType = cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LockAuthenticationScreen(IconAuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray) -> None:\n",
    "        super().__init__(frame, \"./assets/lock.png\", (255, 255, 255), \"PRESS A KEY TO START FACE RECOGNITION\", bg_color=(0, 0, 0), alpha=0.4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessGrantedAuthenticationScreen(IconAuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray) -> None:\n",
    "        super().__init__(frame, \"./assets/success.png\", (0, 255, 0), \"ACCESS GRANTED\", bg_color=(0, 255, 0), alpha = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccessDeniedAuthenticationScreen(IconAuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray) -> None:\n",
    "        super().__init__(frame, \"./assets/danger.png\", (0, 0, 255), \"ACCESS DENIED\", bg_color=(0, 0, 255), alpha = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "class FaceRecognizerAuthenticationScreen(AuthenticationScreen):\n",
    "    def __init__(self, frame: np.ndarray ) -> None:\n",
    "        self.frame = frame\n",
    "\n",
    "    def display(self) -> None:\n",
    "        self.draw_landmarks()\n",
    "        self.draw_processing_text()\n",
    "    \n",
    "    def draw_processing_text(self):\n",
    "        font_weight=2\n",
    "        font_size=1.2\n",
    "        text = \"PROCESSING FACE\"\n",
    "        text_size, _ = cv.getTextSize(text, cv.FONT_HERSHEY_DUPLEX, font_size, font_weight)\n",
    "        text_width, text_height = text_size\n",
    "        text_x = self.frame.shape[1] // 2 - text_width // 2\n",
    "        text_y = 300 \n",
    "        cv.putText(self.frame, text, (text_x, text_y), cv.FONT_HERSHEY_DUPLEX, font_size, (255, 255, 255), font_weight, lineType = cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    def draw_landmarks(self):\n",
    "        base_options = python.BaseOptions(model_asset_path='./face_landmarker.task')\n",
    "        options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                            output_face_blendshapes=True,\n",
    "                                            output_facial_transformation_matrixes=True)\n",
    "        detector = vision.FaceLandmarker.create_from_options(options)\n",
    "        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=self.frame)\n",
    "        detection_result = detector.detect(mp_image)\n",
    "        self.frame[::] = self.draw_landmarks_on_image(self.frame, detection_result)\n",
    "    \n",
    "    def draw_landmarks_on_image(self, rgb_image, detection_result):\n",
    "        face_landmarks_list = detection_result.face_landmarks\n",
    "        annotated_image = np.copy(rgb_image)\n",
    "\n",
    "        # Loop through the detected faces to visualize.\n",
    "        for idx in range(len(face_landmarks_list)):\n",
    "            face_landmarks = face_landmarks_list[idx]\n",
    "\n",
    "            # Draw the face landmarks.\n",
    "            face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "            face_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "            ])\n",
    "\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles\n",
    "                .get_default_face_mesh_tesselation_style())\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles\n",
    "                .get_default_face_mesh_contours_style())\n",
    "            solutions.drawing_utils.draw_landmarks(\n",
    "                image=annotated_image,\n",
    "                landmark_list=face_landmarks_proto,\n",
    "                connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp.solutions.drawing_styles\n",
    "                .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "        return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputManager:\n",
    "    @staticmethod\n",
    "    def is_space_pressed() -> bool:\n",
    "        return cv.waitKey(1) & 0xFF == ord(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import threading\n",
    "\n",
    "class MockFaceComparator(FaceComparator):\n",
    "    def compare(self, first_face : FaceDetectorResult, second_face : FaceDetectorResult) -> FaceComparatorResult:\n",
    "        return FaceComparatorResult(1)\n",
    "\n",
    "class FaceAuthorizator:\n",
    "    def __init__(self, face_detector: FaceDetector, face_comparator: FaceComparator, base_person : Person):\n",
    "        self.is_authorizated = False\n",
    "        self.finished_recognition = False\n",
    "        self.face_comparator = face_comparator\n",
    "        self.face_detector = face_detector\n",
    "    \n",
    "    def load_person() -> Person:\n",
    "        pass\n",
    "\n",
    "    def set_authorizated(self, frame):\n",
    "        detected_person = self.face_detector.detect(frame)\n",
    "        base_person = \n",
    "        result = self.face_comparator.compare(detected_person, self.load_person())\n",
    "        self.is_authorizated = True if result.similarity > 0.8 else False\n",
    "        self.finished_recognition=True\n",
    "    \n",
    "    def start_authorization(self, frame):\n",
    "        timer = threading.Timer(5.0, lambda  : self.set_authorizated(frame))\n",
    "        timer.start()\n",
    "\n",
    "    def is_access_denied(self) -> bool:\n",
    "        return not self.is_authorizated and self.finished_recognition\n",
    "    \n",
    "    def is_access_granted(self) -> bool:\n",
    "        return self.is_authorizated and self.finished_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STATE MACHINE CREATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1701817617.331307       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817617.331767       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817617.454666       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817617.455151       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817617.586692       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817617.587088       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817617.722757       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817617.723278       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817617.863079       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817617.863522       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817617.988644       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817617.989137       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817618.122225       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817618.122686       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817618.255538       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817618.256006       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817618.386578       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817618.386968       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817618.522736       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817618.523286       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817618.655451       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817618.655920       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817618.788355       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817618.788825       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817618.921940       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817618.922412       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817619.055488       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817619.055941       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817619.188173       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817619.188634       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817619.320875       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817619.321326       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817619.453051       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817619.453510       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817619.588428       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817619.588885       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817619.721236       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817619.721694       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817619.854514       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817619.854976       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817619.988329       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817619.988797       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817620.121391       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817620.121849       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817620.255092       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817620.255562       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817620.388092       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817620.388543       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817620.521374       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817620.521848       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817620.654654       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817620.655123       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817620.787402       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817620.787867       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817620.920499       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817620.920975       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817621.054269       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817621.054730       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817621.187490       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817621.187958       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817621.321060       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817621.321526       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817621.454638       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817621.455188       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817621.587399       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817621.587850       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817621.720411       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817621.720864       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817621.854222       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817621.854685       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817621.987175       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817621.987652       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817622.121031       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817622.121529       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n",
      "I0000 00:00:1701817622.253673       1 gl_context.cc:344] GL version: 2.1 (2.1 ATI-4.12.7), renderer: AMD Radeon Pro 5300M OpenGL Engine\n",
      "W0000 00:00:1701817622.254151       1 face_landmarker_graph.cc:174] Sets FaceBlendshapesGraph acceleration to xnnpack by default.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 32\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39mread()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     state_machine\u001b[39m.\u001b[39;49mexecute(frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     cv\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39mVideo\u001b[39m\u001b[39m'\u001b[39m, frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mif\u001b[39;00m cv\u001b[39m.\u001b[39mwaitKey(\u001b[39m1\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexecute\u001b[39m(\u001b[39mself\u001b[39m, frame : np\u001b[39m.\u001b[39marray):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state\u001b[39m.\u001b[39mscreen(frame)\u001b[39m.\u001b[39mdisplay()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_conditions(frame)\n",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 32\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_conditions\u001b[39m(\u001b[39mself\u001b[39m, frame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mfor\u001b[39;00m transition \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state\u001b[39m.\u001b[39mtransitions:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39mif\u001b[39;00m transition\u001b[39m.\u001b[39;49mevaluate_transition():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state\u001b[39m.\u001b[39mon_exit(frame)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m transition\u001b[39m.\u001b[39mnext_state, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstates))[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 32\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_transition\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mif\u001b[39;00m action():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32m/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb Celda 32\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_space_pressed\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/josepenaseco/Desktop/dev/vc/p6/p6_vc.ipynb#Y120sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cv\u001b[39m.\u001b[39;49mwaitKey(\u001b[39m1\u001b[39;49m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "\n",
    "video = cv.VideoCapture(0)\n",
    "authorizator = FaceAuthorizator(MockFaceComparator())\n",
    "\n",
    "state_machine = StateMachine.start_building().add_state(\n",
    "        State.default(\n",
    "            name=\"LOCK\", \n",
    "            screen=LockAuthenticationScreen)\n",
    "                    .do(Transition.to(\"RECOGNIZING\").when(InputManager.is_space_pressed))\n",
    "    ).add_state(\n",
    "        State.of( \n",
    "            name=\"RECOGNIZING\", \n",
    "            screen=FaceRecognizerAuthenticationScreen)\n",
    "                .do(Transition.to(\"GRANTED\").when(authorizator.is_access_granted))\n",
    "                .do(Transition.to(\"DENIED\").when(authorizator.is_access_denied))\n",
    "                .do_on_enter(authorizator.start_authorization)\n",
    "    ).add_state(\n",
    "        State.of(\n",
    "            name=\"GRANTED\", \n",
    "            screen =AccessGrantedAuthenticationScreen)\n",
    "                    .do(Transition.to(\"LOCK\").when(InputManager.is_space_pressed))\n",
    "    ).add_state(\n",
    "\n",
    "    State.of(\n",
    "          name=\"DENIED\", \n",
    "          screen=AccessDeniedAuthenticationScreen)\n",
    "            .do(Transition.to(\"LOCK\").when(InputManager.is_space_pressed))\n",
    "    ).build()\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    state_machine.execute(frame)\n",
    "    cv.imshow('Video', frame)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
